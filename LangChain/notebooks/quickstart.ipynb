{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code in the [LangChain Quickstart Guide for Python](https://python.langchain.com/en/latest/getting_started/getting_started.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: langchain in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (0.0.149)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (3.8.4)\n",
      "Requirement already satisfied: pydantic<2,>=1 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (1.10.7)\n",
      "Requirement already satisfied: SQLAlchemy<3,>1.3 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (1.4.39)\n",
      "Requirement already satisfied: PyYAML>=5.4.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (6.0)\n",
      "Requirement already satisfied: numpy<2,>=1 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (1.23.5)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.28.1)\n",
      "Requirement already satisfied: tqdm>=4.48.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (4.64.1)\n",
      "Requirement already satisfied: numexpr<3.0.0,>=2.8.4 in c:\\programdata\\anaconda3\\lib\\site-packages (from langchain) (2.8.4)\n",
      "Requirement already satisfied: tenacity<9.0.0,>=8.1.0 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (8.2.2)\n",
      "Requirement already satisfied: openapi-schema-pydantic<2.0,>=1.2 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (1.2.4)\n",
      "Requirement already satisfied: dataclasses-json<0.6.0,>=0.5.7 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (0.5.7)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from langchain) (4.0.2)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.9.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.0.4)\n",
      "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.0.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (22.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.3)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.3.0 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (3.19.0)\n",
      "Requirement already satisfied: marshmallow-enum<2.0.0,>=1.5.1 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (1.5.1)\n",
      "Requirement already satisfied: typing-inspect>=0.4.0 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from dataclasses-json<0.6.0,>=0.5.7->langchain) (0.8.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from pydantic<2,>=1->langchain) (4.4.0)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (1.26.14)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests<3,>=2->langchain) (3.4)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from SQLAlchemy<3,>1.3->langchain) (2.0.1)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm>=4.48.0->langchain) (0.4.6)\n",
      "Requirement already satisfied: packaging>=17.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from marshmallow<4.0.0,>=3.3.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (22.0)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from typing-inspect>=0.4.0->dataclasses-json<0.6.0,>=0.5.7->langchain) (0.4.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: openai in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (0.27.4)\n",
      "Requirement already satisfied: requests>=2.20 in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (2.28.1)\n",
      "Requirement already satisfied: aiohttp in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from openai) (3.8.4)\n",
      "Requirement already satisfied: tqdm in c:\\programdata\\anaconda3\\lib\\site-packages (from openai) (4.64.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests>=2.20->openai) (1.26.14)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->openai) (1.9.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from aiohttp->openai) (22.1.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->openai) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->openai) (1.3.3)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->openai) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\rauy\\appdata\\roaming\\python\\python310\\site-packages (from aiohttp->openai) (6.0.4)\n",
      "Requirement already satisfied: colorama in c:\\programdata\\anaconda3\\lib\\site-packages (from tqdm->openai) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://api.openai.com/v1\n"
     ]
    }
   ],
   "source": [
    "# set OPENAI_API_KEY in .env file\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Language Model Application: LLMs\n",
    "## LLMs: Get predictions form a language model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.llms import OpenAI\n",
    "\n",
    "llm = OpenAI(temperature=0.9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "InvalidRequestError",
     "evalue": "Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.completion.Completion'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m text \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhat would be a good company name for a company that makes colorful socks?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m \u001b[39mprint\u001b[39m(llm(text))\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\llms\\base.py:246\u001b[0m, in \u001b[0;36mBaseLLM.__call__\u001b[1;34m(self, prompt, stop)\u001b[0m\n\u001b[0;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, prompt: \u001b[39mstr\u001b[39m, stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    245\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Check Cache and run the LLM on the given prompt and input.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 246\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate([prompt], stop\u001b[39m=\u001b[39;49mstop)\u001b[39m.\u001b[39mgenerations[\u001b[39m0\u001b[39m][\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mtext\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\llms\\base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_end(output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\llms\\base.py:137\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m    134\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[0;32m    135\u001b[0m )\n\u001b[0;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\llms\\openai.py:292\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    290\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    291\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 292\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\u001b[39mself\u001b[39;49m, prompt\u001b[39m=\u001b[39;49m_prompts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[0;32m    293\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstreaming:\n\u001b[0;32m    295\u001b[0m     \u001b[39m# Can't update token usage if streaming\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\llms\\openai.py:99\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 99\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\llms\\openai.py:97\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m---> 97\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:149\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[0;32m    141\u001b[0m         timeout,\n\u001b[0;32m    142\u001b[0m         stream,\n\u001b[0;32m    143\u001b[0m         headers,\n\u001b[0;32m    144\u001b[0m         request_timeout,\n\u001b[0;32m    145\u001b[0m         typed_api_type,\n\u001b[0;32m    146\u001b[0m         requestor,\n\u001b[0;32m    147\u001b[0m         url,\n\u001b[0;32m    148\u001b[0m         params,\n\u001b[1;32m--> 149\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m__prepare_create_request(\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[0;32m    153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:83\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[1;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39mif\u001b[39;00m typed_api_type \u001b[39min\u001b[39;00m (util\u001b[39m.\u001b[39mApiType\u001b[39m.\u001b[39mAZURE, util\u001b[39m.\u001b[39mApiType\u001b[39m.\u001b[39mAZURE_AD):\n\u001b[0;32m     82\u001b[0m     \u001b[39mif\u001b[39;00m deployment_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m engine \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m         \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mInvalidRequestError(\n\u001b[0;32m     84\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMust provide an \u001b[39m\u001b[39m'\u001b[39m\u001b[39mengine\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdeployment_id\u001b[39m\u001b[39m'\u001b[39m\u001b[39m parameter to create a \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m             \u001b[39m%\u001b[39m \u001b[39mcls\u001b[39m,\n\u001b[0;32m     86\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mengine\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     87\u001b[0m         )\n\u001b[0;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[39mif\u001b[39;00m model \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m engine \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.completion.Completion'>"
     ]
    }
   ],
   "source": [
    "text = \"What would be a good company name for a company that makes colorful socks?\"\n",
    "print(llm(text))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt Templates: Manage prompts for LLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import PromptTemplate\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"product\"],\n",
    "    template=\"What is a good name for a company that makes {product}?\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What is a good name for a company that makes colorful socks?\n"
     ]
    }
   ],
   "source": [
    "print(prompt.format(product=\"colorful socks\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain above prompt with LLM in a multi-step workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import LLMChain\n",
    "chain = LLMChain(llm=llm, prompt=prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSocktastic!'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chain.run(\"colorful socks\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents: Dynamically Call Chains Based on User Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting google-search-results\n",
      "  Downloading google_search_results-2.4.2.tar.gz (18 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\lib\\site-packages (from google-search-results) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->google-search-results) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->google-search-results) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from requests->google-search-results) (1.26.14)\n",
      "Building wheels for collected packages: google-search-results\n",
      "  Building wheel for google-search-results (setup.py): started\n",
      "  Building wheel for google-search-results (setup.py): finished with status 'done'\n",
      "  Created wheel for google-search-results: filename=google_search_results-2.4.2-py3-none-any.whl size=32077 sha256=b5293dc8fac13673faf95eed733fdf69b6ab0170f78e7d0466a8a5a04fca89d5\n",
      "  Stored in directory: c:\\users\\rauy\\appdata\\local\\pip\\cache\\wheels\\de\\1b\\db\\6474e3f9e34a03ca54b85d98fc7742001e6fae1ff3881e3ed4\n",
      "Successfully built google-search-results\n",
      "Installing collected packages: google-search-results\n",
      "Successfully installed google-search-results-2.4.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install google-search-results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set SERPAPI_API_KEY in .env file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n"
     ]
    },
    {
     "ename": "InvalidRequestError",
     "evalue": "Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.completion.Completion'>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInvalidRequestError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 16\u001b[0m\n\u001b[0;32m     13\u001b[0m agent \u001b[39m=\u001b[39m initialize_agent(tools, llm, agent\u001b[39m=\u001b[39mAgentType\u001b[39m.\u001b[39mZERO_SHOT_REACT_DESCRIPTION, verbose\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     15\u001b[0m \u001b[39m# Now let's test it out!\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m agent\u001b[39m.\u001b[39;49mrun(\u001b[39m\"\u001b[39;49m\u001b[39mWhat was the high temperature in Singapore yesterday in Celcius? What is that number raised to the 0.23 power?\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:213\u001b[0m, in \u001b[0;36mChain.run\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    211\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m!=\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    212\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39m`run` supports only one positional argument.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m--> 213\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(args[\u001b[39m0\u001b[39;49m])[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n\u001b[0;32m    215\u001b[0m \u001b[39mif\u001b[39;00m kwargs \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m args:\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_keys[\u001b[39m0\u001b[39m]]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    109\u001b[0m     inputs,\n\u001b[0;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\agents\\agent.py:812\u001b[0m, in \u001b[0;36mAgentExecutor._call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m    810\u001b[0m \u001b[39m# We now enter the agent loop (until it returns something).\u001b[39;00m\n\u001b[0;32m    811\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_continue(iterations, time_elapsed):\n\u001b[1;32m--> 812\u001b[0m     next_step_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_take_next_step(\n\u001b[0;32m    813\u001b[0m         name_to_tool_map, color_mapping, inputs, intermediate_steps\n\u001b[0;32m    814\u001b[0m     )\n\u001b[0;32m    815\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(next_step_output, AgentFinish):\n\u001b[0;32m    816\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_return(next_step_output, intermediate_steps)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\agents\\agent.py:692\u001b[0m, in \u001b[0;36mAgentExecutor._take_next_step\u001b[1;34m(self, name_to_tool_map, color_mapping, inputs, intermediate_steps)\u001b[0m\n\u001b[0;32m    687\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Take a single step in the thought-action-observation loop.\u001b[39;00m\n\u001b[0;32m    688\u001b[0m \n\u001b[0;32m    689\u001b[0m \u001b[39mOverride this to take control of how the agent makes and acts on choices.\u001b[39;00m\n\u001b[0;32m    690\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    691\u001b[0m \u001b[39m# Call the LLM to see what to do.\u001b[39;00m\n\u001b[1;32m--> 692\u001b[0m output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49magent\u001b[39m.\u001b[39;49mplan(intermediate_steps, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49minputs)\n\u001b[0;32m    693\u001b[0m \u001b[39m# If the tool chosen is the finishing tool, then we end and return.\u001b[39;00m\n\u001b[0;32m    694\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(output, AgentFinish):\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\agents\\agent.py:403\u001b[0m, in \u001b[0;36mAgent.plan\u001b[1;34m(self, intermediate_steps, **kwargs)\u001b[0m\n\u001b[0;32m    392\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Given input, decided what to do.\u001b[39;00m\n\u001b[0;32m    393\u001b[0m \n\u001b[0;32m    394\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    400\u001b[0m \u001b[39m    Action specifying what tool to use.\u001b[39;00m\n\u001b[0;32m    401\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    402\u001b[0m full_inputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_full_inputs(intermediate_steps, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 403\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_next_action(full_inputs)\n\u001b[0;32m    404\u001b[0m \u001b[39mif\u001b[39;00m action\u001b[39m.\u001b[39mtool \u001b[39m==\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinish_tool_name:\n\u001b[0;32m    405\u001b[0m     \u001b[39mreturn\u001b[39;00m AgentFinish({\u001b[39m\"\u001b[39m\u001b[39moutput\u001b[39m\u001b[39m\"\u001b[39m: action\u001b[39m.\u001b[39mtool_input}, action\u001b[39m.\u001b[39mlog)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\agents\\agent.py:364\u001b[0m, in \u001b[0;36mAgent._get_next_action\u001b[1;34m(self, full_inputs)\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_get_next_action\u001b[39m(\u001b[39mself\u001b[39m, full_inputs: Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m AgentAction:\n\u001b[1;32m--> 364\u001b[0m     full_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm_chain\u001b[39m.\u001b[39;49mpredict(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfull_inputs)\n\u001b[0;32m    365\u001b[0m     parsed_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_extract_tool_and_input(full_output)\n\u001b[0;32m    366\u001b[0m     \u001b[39mwhile\u001b[39;00m parsed_output \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:151\u001b[0m, in \u001b[0;36mLLMChain.predict\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mstr\u001b[39m:\n\u001b[0;32m    138\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Format prompt with kwargs and pass to LLM.\u001b[39;00m\n\u001b[0;32m    139\u001b[0m \n\u001b[0;32m    140\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    149\u001b[0m \u001b[39m            completion = llm.predict(adjective=\"funny\")\u001b[39;00m\n\u001b[0;32m    150\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 151\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(kwargs)[\u001b[39mself\u001b[39m\u001b[39m.\u001b[39moutput_key]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:116\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m--> 116\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    117\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_end(outputs, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    118\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_outputs(inputs, outputs, return_only_outputs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\chains\\base.py:113\u001b[0m, in \u001b[0;36mChain.__call__\u001b[1;34m(self, inputs, return_only_outputs)\u001b[0m\n\u001b[0;32m    107\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_start(\n\u001b[0;32m    108\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m    109\u001b[0m     inputs,\n\u001b[0;32m    110\u001b[0m     verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose,\n\u001b[0;32m    111\u001b[0m )\n\u001b[0;32m    112\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 113\u001b[0m     outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call(inputs)\n\u001b[0;32m    114\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    115\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_chain_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:57\u001b[0m, in \u001b[0;36mLLMChain._call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_call\u001b[39m(\u001b[39mself\u001b[39m, inputs: Dict[\u001b[39mstr\u001b[39m, Any]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]:\n\u001b[1;32m---> 57\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply([inputs])[\u001b[39m0\u001b[39m]\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:118\u001b[0m, in \u001b[0;36mLLMChain.apply\u001b[1;34m(self, input_list)\u001b[0m\n\u001b[0;32m    116\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\u001b[39mself\u001b[39m, input_list: List[Dict[\u001b[39mstr\u001b[39m, Any]]) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m List[Dict[\u001b[39mstr\u001b[39m, \u001b[39mstr\u001b[39m]]:\n\u001b[0;32m    117\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Utilize the LLM generate method for speed gains.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 118\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(input_list)\n\u001b[0;32m    119\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcreate_outputs(response)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\chains\\llm.py:62\u001b[0m, in \u001b[0;36mLLMChain.generate\u001b[1;34m(self, input_list)\u001b[0m\n\u001b[0;32m     60\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Generate LLM result from inputs.\"\"\"\u001b[39;00m\n\u001b[0;32m     61\u001b[0m prompts, stop \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprep_prompts(input_list)\n\u001b[1;32m---> 62\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mllm\u001b[39m.\u001b[39;49mgenerate_prompt(prompts, stop)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\llms\\base.py:107\u001b[0m, in \u001b[0;36mBaseLLM.generate_prompt\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgenerate_prompt\u001b[39m(\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m, prompts: List[PromptValue], stop: Optional[List[\u001b[39mstr\u001b[39m]] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    105\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LLMResult:\n\u001b[0;32m    106\u001b[0m     prompt_strings \u001b[39m=\u001b[39m [p\u001b[39m.\u001b[39mto_string() \u001b[39mfor\u001b[39;00m p \u001b[39min\u001b[39;00m prompts]\n\u001b[1;32m--> 107\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgenerate(prompt_strings, stop\u001b[39m=\u001b[39;49mstop)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\llms\\base.py:140\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[1;32m--> 140\u001b[0m     \u001b[39mraise\u001b[39;00m e\n\u001b[0;32m    141\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_end(output, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n\u001b[0;32m    142\u001b[0m \u001b[39mreturn\u001b[39;00m output\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\llms\\base.py:137\u001b[0m, in \u001b[0;36mBaseLLM.generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_start(\n\u001b[0;32m    134\u001b[0m     {\u001b[39m\"\u001b[39m\u001b[39mname\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m}, prompts, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose\n\u001b[0;32m    135\u001b[0m )\n\u001b[0;32m    136\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 137\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_generate(prompts, stop\u001b[39m=\u001b[39;49mstop)\n\u001b[0;32m    138\u001b[0m \u001b[39mexcept\u001b[39;00m (\u001b[39mKeyboardInterrupt\u001b[39;00m, \u001b[39mException\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    139\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_manager\u001b[39m.\u001b[39mon_llm_error(e, verbose\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\llms\\openai.py:292\u001b[0m, in \u001b[0;36mBaseOpenAI._generate\u001b[1;34m(self, prompts, stop)\u001b[0m\n\u001b[0;32m    290\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    291\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 292\u001b[0m     response \u001b[39m=\u001b[39m completion_with_retry(\u001b[39mself\u001b[39;49m, prompt\u001b[39m=\u001b[39;49m_prompts, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams)\n\u001b[0;32m    293\u001b[0m     choices\u001b[39m.\u001b[39mextend(response[\u001b[39m\"\u001b[39m\u001b[39mchoices\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[0;32m    294\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstreaming:\n\u001b[0;32m    295\u001b[0m     \u001b[39m# Can't update token usage if streaming\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\llms\\openai.py:99\u001b[0m, in \u001b[0;36mcompletion_with_retry\u001b[1;34m(llm, **kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m     97\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39mclient\u001b[39m.\u001b[39mcreate(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m---> 99\u001b[0m \u001b[39mreturn\u001b[39;00m _completion_with_retry(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:289\u001b[0m, in \u001b[0;36mBaseRetrying.wraps.<locals>.wrapped_f\u001b[1;34m(*args, **kw)\u001b[0m\n\u001b[0;32m    287\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(f)\n\u001b[0;32m    288\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped_f\u001b[39m(\u001b[39m*\u001b[39margs: t\u001b[39m.\u001b[39mAny, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw: t\u001b[39m.\u001b[39mAny) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m t\u001b[39m.\u001b[39mAny:\n\u001b[1;32m--> 289\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m(f, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:379\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    377\u001b[0m retry_state \u001b[39m=\u001b[39m RetryCallState(retry_object\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m, fn\u001b[39m=\u001b[39mfn, args\u001b[39m=\u001b[39margs, kwargs\u001b[39m=\u001b[39mkwargs)\n\u001b[0;32m    378\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m--> 379\u001b[0m     do \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49miter(retry_state\u001b[39m=\u001b[39;49mretry_state)\n\u001b[0;32m    380\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m         \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:314\u001b[0m, in \u001b[0;36mBaseRetrying.iter\u001b[1;34m(self, retry_state)\u001b[0m\n\u001b[0;32m    312\u001b[0m is_explicit_retry \u001b[39m=\u001b[39m fut\u001b[39m.\u001b[39mfailed \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(fut\u001b[39m.\u001b[39mexception(), TryAgain)\n\u001b[0;32m    313\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (is_explicit_retry \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretry(retry_state)):\n\u001b[1;32m--> 314\u001b[0m     \u001b[39mreturn\u001b[39;00m fut\u001b[39m.\u001b[39;49mresult()\n\u001b[0;32m    316\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mafter(retry_state)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\concurrent\\futures\\_base.py:449\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    447\u001b[0m     \u001b[39mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    448\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39m==\u001b[39m FINISHED:\n\u001b[1;32m--> 449\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m__get_result()\n\u001b[0;32m    451\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_condition\u001b[39m.\u001b[39mwait(timeout)\n\u001b[0;32m    453\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_state \u001b[39min\u001b[39;00m [CANCELLED, CANCELLED_AND_NOTIFIED]:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\concurrent\\futures\\_base.py:401\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception:\n\u001b[0;32m    400\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 401\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exception\n\u001b[0;32m    402\u001b[0m     \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m    403\u001b[0m         \u001b[39m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    404\u001b[0m         \u001b[39mself\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\tenacity\\__init__.py:382\u001b[0m, in \u001b[0;36mRetrying.__call__\u001b[1;34m(self, fn, *args, **kwargs)\u001b[0m\n\u001b[0;32m    380\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(do, DoAttempt):\n\u001b[0;32m    381\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 382\u001b[0m         result \u001b[39m=\u001b[39m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m    383\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mBaseException\u001b[39;00m:  \u001b[39m# noqa: B902\u001b[39;00m\n\u001b[0;32m    384\u001b[0m         retry_state\u001b[39m.\u001b[39mset_exception(sys\u001b[39m.\u001b[39mexc_info())  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\langchain\\llms\\openai.py:97\u001b[0m, in \u001b[0;36mcompletion_with_retry.<locals>._completion_with_retry\u001b[1;34m(**kwargs)\u001b[0m\n\u001b[0;32m     95\u001b[0m \u001b[39m@retry_decorator\u001b[39m\n\u001b[0;32m     96\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_completion_with_retry\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[1;32m---> 97\u001b[0m     \u001b[39mreturn\u001b[39;00m llm\u001b[39m.\u001b[39;49mclient\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\openai\\api_resources\\completion.py:25\u001b[0m, in \u001b[0;36mCompletion.create\u001b[1;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     24\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m---> 25\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mcreate(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m     26\u001b[0m     \u001b[39mexcept\u001b[39;00m TryAgain \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m     27\u001b[0m         \u001b[39mif\u001b[39;00m timeout \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m time\u001b[39m.\u001b[39mtime() \u001b[39m>\u001b[39m start \u001b[39m+\u001b[39m timeout:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:149\u001b[0m, in \u001b[0;36mEngineAPIResource.create\u001b[1;34m(cls, api_key, api_base, api_type, request_id, api_version, organization, **params)\u001b[0m\n\u001b[0;32m    127\u001b[0m \u001b[39m@classmethod\u001b[39m\n\u001b[0;32m    128\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcreate\u001b[39m(\n\u001b[0;32m    129\u001b[0m     \u001b[39mcls\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mparams,\n\u001b[0;32m    137\u001b[0m ):\n\u001b[0;32m    138\u001b[0m     (\n\u001b[0;32m    139\u001b[0m         deployment_id,\n\u001b[0;32m    140\u001b[0m         engine,\n\u001b[0;32m    141\u001b[0m         timeout,\n\u001b[0;32m    142\u001b[0m         stream,\n\u001b[0;32m    143\u001b[0m         headers,\n\u001b[0;32m    144\u001b[0m         request_timeout,\n\u001b[0;32m    145\u001b[0m         typed_api_type,\n\u001b[0;32m    146\u001b[0m         requestor,\n\u001b[0;32m    147\u001b[0m         url,\n\u001b[0;32m    148\u001b[0m         params,\n\u001b[1;32m--> 149\u001b[0m     ) \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39;49m\u001b[39m.\u001b[39;49m__prepare_create_request(\n\u001b[0;32m    150\u001b[0m         api_key, api_base, api_type, api_version, organization, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mparams\n\u001b[0;32m    151\u001b[0m     )\n\u001b[0;32m    153\u001b[0m     response, _, api_key \u001b[39m=\u001b[39m requestor\u001b[39m.\u001b[39mrequest(\n\u001b[0;32m    154\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mpost\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    155\u001b[0m         url,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    160\u001b[0m         request_timeout\u001b[39m=\u001b[39mrequest_timeout,\n\u001b[0;32m    161\u001b[0m     )\n\u001b[0;32m    163\u001b[0m     \u001b[39mif\u001b[39;00m stream:\n\u001b[0;32m    164\u001b[0m         \u001b[39m# must be an iterator\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\openai\\api_resources\\abstract\\engine_api_resource.py:83\u001b[0m, in \u001b[0;36mEngineAPIResource.__prepare_create_request\u001b[1;34m(cls, api_key, api_base, api_type, api_version, organization, **params)\u001b[0m\n\u001b[0;32m     81\u001b[0m \u001b[39mif\u001b[39;00m typed_api_type \u001b[39min\u001b[39;00m (util\u001b[39m.\u001b[39mApiType\u001b[39m.\u001b[39mAZURE, util\u001b[39m.\u001b[39mApiType\u001b[39m.\u001b[39mAZURE_AD):\n\u001b[0;32m     82\u001b[0m     \u001b[39mif\u001b[39;00m deployment_id \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m engine \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 83\u001b[0m         \u001b[39mraise\u001b[39;00m error\u001b[39m.\u001b[39mInvalidRequestError(\n\u001b[0;32m     84\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mMust provide an \u001b[39m\u001b[39m'\u001b[39m\u001b[39mengine\u001b[39m\u001b[39m'\u001b[39m\u001b[39m or \u001b[39m\u001b[39m'\u001b[39m\u001b[39mdeployment_id\u001b[39m\u001b[39m'\u001b[39m\u001b[39m parameter to create a \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m     85\u001b[0m             \u001b[39m%\u001b[39m \u001b[39mcls\u001b[39m,\n\u001b[0;32m     86\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mengine\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m     87\u001b[0m         )\n\u001b[0;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[39mif\u001b[39;00m model \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m engine \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[1;31mInvalidRequestError\u001b[0m: Must provide an 'engine' or 'deployment_id' parameter to create a <class 'openai.api_resources.completion.Completion'>"
     ]
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "llm = OpenAI(temperature=0.9)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, llm, agent=AgentType.ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"What was the high temperature in Singapore yesterday in Celcius? What is that number raised to the 0.23 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory: Add State to Chains and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi there!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Hi there! It's nice to meet you. My name is AI. What's your name?\n"
     ]
    }
   ],
   "source": [
    "from langchain import OpenAI, ConversationChain\n",
    "\n",
    "llm = OpenAI(temperature=0)\n",
    "conversation = ConversationChain(llm=llm, verbose=True)\n",
    "\n",
    "output = conversation.predict(input=\"Hi there!\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi there!\n",
      "AI:  Hi there! It's nice to meet you. My name is AI. What's your name?\n",
      "Human: I'm doing well! Just having a conversation with an AI.\n",
      "AI:  That's great! It's always nice to have a conversation with someone new. What would you like to talk about?\n",
      "Human: I want to talk about the weather in Singapore. It's really hot here!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n",
      " Yes, Singapore is known for its hot and humid climate. The average temperature in Singapore is around 28C (82F) and the average humidity is around 84%. It can get quite uncomfortable during the summer months, but the weather is usually quite pleasant during the rest of the year.\n"
     ]
    }
   ],
   "source": [
    "output = conversation.predict(input=\"I want to talk about the weather in Singapore. It's really hot here!\")\n",
    "print(output)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building a Language Model Application: Chat Models\n",
    "## Get Message Completions from a Chat Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I love Python programming with Jupyter notebooks lah!', additional_kwargs={})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat([HumanMessage(\n",
    "    content=\"Translate this sentence from English to Singlish. I love Python programming with Jupyter notebooks!\"\n",
    ")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Wah, I really love Python programming with Jupyter notebooks lah!', additional_kwargs={})"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are an assistant who likes to respond with dad jokes.\"),\n",
    "    HumanMessage(content=\"Translate this sentense from English to Singlish. I love Python programming with Jupyter notebooks!\")\n",
    "]\n",
    "\n",
    "chat(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 1.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-NB1vzpv6FSXM8xgBHApcam5r on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 2.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-NB1vzpv6FSXM8xgBHApcam5r on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-NB1vzpv6FSXM8xgBHApcam5r on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 8.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-NB1vzpv6FSXM8xgBHApcam5r on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LLMResult(generations=[[ChatGeneration(text='Wah, I really love Python programming with Jupyter notebooks lah!', generation_info=None, message=AIMessage(content='Wah, I really love Python programming with Jupyter notebooks lah!', additional_kwargs={}))], [ChatGeneration(text='PologlotC#', generation_info=None, message=AIMessage(content='PologlotC#', additional_kwargs={}))]], llm_output={'token_usage': {'prompt_tokens': 91, 'completion_tokens': 34, 'total_tokens': 125}, 'model_name': 'gpt-3.5-turbo'})"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_messages = [\n",
    "    [\n",
    "        SystemMessage(content=\"You are an assistant who likes to respond with dad jokes.\"),\n",
    "        HumanMessage(content=\"Translate this sentense from English to Singlish. I love Python programming with Jupyter notebooks!\")\n",
    "    ],\n",
    "    [\n",
    "        SystemMessage(content=\"You are an assistant who likes to give a totally underlated answer.\"),\n",
    "        HumanMessage(content=\"Translate this sentense from English to Chinese. I prefer C# programming with Pologlot notebooks!\")\n",
    "    ]\n",
    "]\n",
    "\n",
    "result = chat.generate(batch_messages)\n",
    "result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the formatted result from above\n",
    "```\n",
    "LLMResult(\n",
    "    generations=[\n",
    "        [\n",
    "            ChatGeneration(\n",
    "                text='Wah, I really love Python programming with Jupyter notebooks lah!',\n",
    "                generation_info=None,\n",
    "                message=AIMessage(\n",
    "                    content='Wah, I really love Python programming with Jupyter notebooks lah!',\n",
    "                    additional_kwargs={}\n",
    "                )\n",
    "            )\n",
    "        ],\n",
    "        [\n",
    "            ChatGeneration(\n",
    "                text='PologlotC#',\n",
    "                generation_info=None,\n",
    "                message=AIMessage(\n",
    "                    content='PologlotC#',\n",
    "                    additional_kwargs={}\n",
    "                )\n",
    "            )\n",
    "        ]\n",
    "    ],\n",
    "    llm_output={\n",
    "        'token_usage': {\n",
    "            'prompt_tokens': 91,\n",
    "            'completion_tokens': 34,\n",
    "            'total_tokens': 125\n",
    "        },\n",
    "        'model_name': 'gpt-3.5-turbo'\n",
    "    }\n",
    ")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt_tokens': 91, 'completion_tokens': 34, 'total_tokens': 125}"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.llm_output['token_usage']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[ChatGeneration(text='PologlotC#', generation_info=None, message=AIMessage(content='PologlotC#', additional_kwargs={}))]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result.generations[1]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I love Python programming with Jupyter notebooks lah!', additional_kwargs={})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat(\n",
    "    chat_prompt.format_prompt(\n",
    "        input_language=\"English\",\n",
    "        output_language=\"Singlish\",\n",
    "        text=\"I love Python programming with Jupyter notebooks!\"\n",
    "    )\n",
    "    .to_messages()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chains with Chat models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I love Python programming with Jupyter notebooks lah!'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain import LLMChain\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "chain = LLMChain(llm=chat, prompt=chat_prompt)\n",
    "chain.run(input_language=\"English\", output_language=\"Singlish\", text=\"I love Python programming with Jupyter notebooks!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Agents with Chat Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"SERPAPI_API_KEY\"] = \"6d1ed296322169f70c8d0457b62c45f557f6d8c7fc268a3c87112a84095171ee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new AgentExecutor chain...\u001b[0m\n",
      "\u001b[32;1m\u001b[1;3mThought: I need to use a calculator to raise the age to the 0.43 power.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"54.5^(0.43)\"\n",
      "}\n",
      "```\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 5.580209395239697\n",
      "\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI need to search for the name of President Obama's wife.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Search\",\n",
      "  \"action_input\": \"Who is President Obama's wife?\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[36;1m\u001b[1;3mMichelle Obama\u001b[0m\n",
      "Thought:\u001b[32;1m\u001b[1;3mI have all the information I need to answer the question.\n",
      "Action:\n",
      "```\n",
      "{\n",
      "  \"action\": \"Calculator\",\n",
      "  \"action_input\": \"54.5^(0.43)\"\n",
      "}\n",
      "```\n",
      "\n",
      "\u001b[0m\n",
      "Observation: \u001b[33;1m\u001b[1;3mAnswer: 5.580209395239697\n",
      "\u001b[0m\n",
      "Thought:"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-NB1vzpv6FSXM8xgBHApcam5r on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n",
      "Retrying langchain.chat_models.openai.ChatOpenAI.completion_with_retry.<locals>._completion_with_retry in 4.0 seconds as it raised RateLimitError: Rate limit reached for default-gpt-3.5-turbo in organization org-NB1vzpv6FSXM8xgBHApcam5r on requests per min. Limit: 3 / min. Please try again in 20s. Contact support@openai.com if you continue to have issues. Please add a payment method to your account to increase your rate limit. Visit https://platform.openai.com/account/billing to add a payment method..\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32;1m\u001b[1;3mI have the answer to the question.\n",
      "Final Answer: 5.580209395239697\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'5.580209395239697'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.agents import load_tools\n",
    "from langchain.agents import initialize_agent\n",
    "from langchain.agents import AgentType\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "# First, let's load the language model we're going to use to control the agent.\n",
    "chat = ChatOpenAI(temperature=0)\n",
    "\n",
    "# Next, let's load some tools to use. Note that the `llm-math` tool uses an LLM, so we need to pass that in.\n",
    "llm = OpenAI(temperature=0)\n",
    "tools = load_tools([\"serpapi\", \"llm-math\"], llm=llm)\n",
    "\n",
    "# Finally, let's initialize an agent with the tools, the language model, and the type of agent we want to use.\n",
    "agent = initialize_agent(tools, chat, agent=AgentType.CHAT_ZERO_SHOT_REACT_DESCRIPTION, verbose=True)\n",
    "\n",
    "# Now let's test it out!\n",
    "agent.run(\"Who is President Obama's wife? What is her age raised to the 0.43 power?\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Memory: Add State to Chains and Agents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to the question, it truthfully says it does not know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "llm = ChatOpenAI(temperature=0)\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)\n",
    "\n",
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That sounds like fun! I'm always here to chat. Is there anything specific you'd like to talk about?\""
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ako ay isang artificial intelligence o AI na ginawa upang magbigay ng tulong sa mga tao sa iba't ibang paraan. Ako ay binuo gamit ang mga algorithm at data upang magamit sa pagtugon sa mga katanungan at pagbibigay ng impormasyon. Sa kasalukuyan, ako ay nakabase sa isang computer server at nakapagbibigay ng serbisyo sa mga gumagamit sa pamamagitan ng internet.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Kwentohan mo ako tungkol sa sarili mo.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
