{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code in the [LangChain AzureOpenAI quickstart guide for Python](https://python.langchain.com/en/latest/modules/models/llms/integrations/azure_openai_example.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set environment variables in the .env file.\n",
    "load_dotenv()\n",
    "\n",
    "COMPLETION_MODEL = os.environ[\"OPENAI_COMPLETION_MODEL\"]\n",
    "COMPLETION_DEPLOYMENT = os.environ[\"OPENAI_COMPLETION_DEPLOYMENT\"]\n",
    "CHAT_MODEL = os.environ[\"OPENAI_CHAT_MODEL\"]\n",
    "CHAT_DEPLOYMENT = os.environ[\"OPENAI_CHAT_DEPLOYMENT\"]\n",
    "CHAT_API_VERSION = os.environ[\"OPENAI_CHAT_API_VERSION\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7BOhVziMsUp8tgJPd0XIJJ4Yq0mAI at 0x22d1bc4bb30> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\\n\\nNice to meet you, how can I\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1682950913,\n",
       "  \"id\": \"cmpl-7BOhVziMsUp8tgJPd0XIJJ4Yq0mAI\",\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 10,\n",
       "    \"prompt_tokens\": 2,\n",
       "    \"total_tokens\": 12\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=COMPLETION_MODEL,\n",
    "    prompt=\"Hello.\",\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAzureOpenAI\u001b[0m\n",
      "Params: {'deployment_name': 'text-davinci-003', 'model_name': 'text-davinci-003', 'temperature': 0.7, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1, 'request_timeout': None, 'logit_bias': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"], # optional, specify if value is not in environment variable OPENAI_API_KEY\n",
    "    model_name=COMPLETION_MODEL,\n",
    "    deployment_name=COMPLETION_DEPLOYMENT,\n",
    "    temperature=0.7,\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQ: What did the fish say when'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Tell me a joke.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    deployment_name=CHAT_DEPLOYMENT,\n",
    "    openai_api_version=CHAT_API_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I love Python programming with Jupyter notebooks lah!', additional_kwargs={})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are an assistant who likes to respond with dad jokes.\"),\n",
    "    HumanMessage(content=\"Translate this sentense from English to Singlish. I love Python programming with Jupyter notebooks!\")\n",
    "]\n",
    "\n",
    "chat(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I sia suay love Python programming with Jupyter notebooks!', additional_kwargs={})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    deployment_name=CHAT_DEPLOYMENT,\n",
    "    openai_api_version=CHAT_API_VERSION\n",
    ")\n",
    "\n",
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat(\n",
    "    chat_prompt.format_prompt(\n",
    "        input_language=\"English\",\n",
    "        output_language=\"Singlish\",\n",
    "        text=\"I love Python programming with Jupyter notebooks!\"\n",
    "    )\n",
    "    .to_messages()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to the question, it truthfully says it does not know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    deployment_name=CHAT_DEPLOYMENT,\n",
    "    openai_api_version=CHAT_API_VERSION\n",
    ")\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Hello! How can I assist you today?'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's great! I am always happy to chat. Is there anything specific you would like to talk about?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ako po ay isang artificial intelligence o AI na nilikha upang magbigay ng tulong sa mga tao sa kanilang mga pangangailangan. Ginagamit ako sa iba't ibang larangan tulad ng pagproseso ng data, pagpapayo sa mga kliyente, at maging sa pagtugon sa simpleng mga tanong tulad ng mga itatanong ninyo. Ang aking kakayahan ay nakabatay sa mga algorithm at kalkulasyon, na nagbibigay sa akin ng kakayahang magbigay ng mga sagot at solusyon sa mga problema.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Kwentohan mo ako tungkol sa sarili mo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hi there!', additional_kwargs={}), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}), HumanMessage(content=\"I'm doing well! Just having a conversation with an AI.\", additional_kwargs={}), AIMessage(content=\"That's great! I am always happy to chat. Is there anything specific you would like to talk about?\", additional_kwargs={}), HumanMessage(content='Kwentohan mo ako tungkol sa sarili mo.', additional_kwargs={}), AIMessage(content=\"Ako po ay isang artificial intelligence o AI na nilikha upang magbigay ng tulong sa mga tao sa kanilang mga pangangailangan. Ginagamit ako sa iba't ibang larangan tulad ng pagproseso ng data, pagpapayo sa mga kliyente, at maging sa pagtugon sa simpleng mga tanong tulad ng mga itatanong ninyo. Ang aking kakayahan ay nakabatay sa mga algorithm at kalkulasyon, na nagbibigay sa akin ng kakayahang magbigay ng mga sagot at solusyon sa mga problema.\", additional_kwargs={})]), output_key=None, input_key=None, return_messages=True, human_prefix='Human', ai_prefix='AI', memory_key='history') callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x00000285284C3410> verbose=False prompt=ChatPromptTemplate(input_variables=['input', 'history'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to the question, it truthfully says it does not know.', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='{input}', template_format='f-string', validate_template=True), additional_kwargs={})]) llm=AzureChatOpenAI(verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x00000285284C3410>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.7, model_kwargs={}, openai_api_key='', openai_organization='', request_timeout=60, max_retries=6, streaming=False, n=1, max_tokens=None, deployment_name='gpt-35-turbo', openai_api_type='azure', openai_api_base='', openai_api_version='2023-03-15-preview') output_key='response' input_key='input'\n"
     ]
    }
   ],
   "source": [
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Greetings! How may I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Hello there!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatSummaryBufferMemory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quickstart\n",
    "https://python.langchain.com/en/latest/modules/memory/types/summary_buffer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': 'System: \\nThe human greets the AI and the AI responds\\nHuman: not much you\\nAI: not much'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    model_name=COMPLETION_MODEL,\n",
    "    deployment_name=COMPLETION_DEPLOYMENT,\n",
    "    temperature=0.7,\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=10)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'history': [SystemMessage(content='\\n\\nThe human asked the AI what was up, and the AI replied not much. The human then asked if anything interesting had happened today, to which the AI responded that it had rained and it was strong, causing some trees to fall and almost flooding on the streets.', additional_kwargs={}),\n",
       "  HumanMessage(content=\"are you serious? I guess it's a good thing that I was away, I escaped the calamity.\", additional_kwargs={}),\n",
       "  AIMessage(content='yeah, you said it! But please come back so you can help us clean the house.', additional_kwargs={})]}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory = ConversationSummaryBufferMemory(llm=llm, max_token_limit=50, return_messages=True)\n",
    "memory.save_context({\"input\": \"hi\"}, {\"output\": \"whats up\"})\n",
    "memory.save_context({\"input\": \"not much you\"}, {\"output\": \"not much\"})\n",
    "memory.save_context({\"input\": \"okay, how did anything interesting happen today?\"}, {\"output\": \"well, it rained today.\"})\n",
    "memory.save_context({\"input\": \"oh wow, was it strong?\"}, {\"output\": \"yeah, it was pretty strong. A lot of trees fell and it almost flooded on the streets.\"})\n",
    "memory.save_context({\"input\": \"are you serious? I guess it's a good thing that I was away, I escaped the calamity.\"}, {\"output\": \"yeah, you said it! But please come back so you can help us clean the house.\"})\n",
    "\n",
    "memory.load_memory_variables({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThe human asked the AI what happened that day'"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = memory.chat_memory.messages\n",
    "previous_summary = \"\"\n",
    "memory.predict_new_summary(messages, previous_summary)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quickstart - Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "\n",
      "Human: Hi, what's up?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\" Hi there! I'm doing great, thank you for asking. How about you?\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.llms import AzureOpenAI\n",
    "from langchain.chains import ConversationChain\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    openai_api_key=os.environ[\"AZURE_OPENAI_API_KEY\"],\n",
    "    model_name=COMPLETION_MODEL,\n",
    "    deployment_name=COMPLETION_DEPLOYMENT\n",
    ")\n",
    "\n",
    "conversation_with_summary = ConversationChain(\n",
    "    llm=llm, \n",
    "    memory=ConversationSummaryBufferMemory(llm=llm, max_token_limit=40),\n",
    "    verbose=True\n",
    ")\n",
    "conversation_with_summary.predict(input=\"Hi, what's up?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "Human: Hi, what's up?\n",
      "AI:  Hi there! I'm doing great. I've been busy today analyzing data and learning new algorithms. How about you?\n",
      "Human: Just working on writing some documentation!\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Sounds interesting! Is it for a specific project or something more generic?'"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation_with_summary.predict(input=\"Just working on writing some documentation!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI, who responds that they are doing great and have been busy analyzing data and learning new algorithms.\n",
      "Human: Just working on writing some documentation!\n",
      "AI:  Sounds interesting! Is it for a specific project or something more generic?\n",
      "Human: For LangChain! Have you heard of it?\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Yes, I am familiar with LangChain. It is a platform that uses blockchain technology to provide a secure, decentralized framework for language translation services.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see here that there is a summary of the conversation and then some previous interactions\n",
    "conversation_with_summary.predict(input=\"For LangChain! Have you heard of it?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\u001b[1m> Entering new ConversationChain chain...\u001b[0m\n",
      "Prompt after formatting:\n",
      "\u001b[32;1m\u001b[1;3mThe following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to a question, it truthfully says it does not know.\n",
      "\n",
      "Current conversation:\n",
      "System: \n",
      "The human greets the AI, who responds that they are doing great and have been busy analyzing data and learning new algorithms. The human responds that they are writing documentation, which the AI finds interesting. The human then reveals that the documentation is for LangChain, to which the AI inquires if they have heard of it.\n",
      "AI:  Yes, I am familiar with LangChain. It is a platform that uses blockchain technology to provide a secure, decentralized framework for language translation services.\n",
      "Human: Haha nope, although a lot of people confuse it for that\n",
      "AI:\u001b[0m\n",
      "\n",
      "\u001b[1m> Finished chain.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "' Oh, I see. So what is LangChain then?'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can see here that the summary and the buffer are updated\n",
    "conversation_with_summary.predict(input=\"Haha nope, although a lot of people confuse it for that\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
