{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook contains the code in the [LangChain AzureOpenAI quickstart guide for Python](https://python.langchain.com/en/latest/modules/models/llms/integrations/azure_openai_example.html)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set environment variables in the .env file.\n",
    "\n",
    "COMPLETION_MODEL = os.environ[\"OPENAI_COMPLETION_MODEL\"]\n",
    "COMPLETION_DEPLOYMENT = os.environ[\"OPENAI_COMPLETION_DEPLOYMENT\"]\n",
    "CHAT_MODEL = os.environ[\"OPENAI_CHAT_MODEL\"]\n",
    "CHAT_DEPLOYMENT = os.environ[\"OPENAI_CHAT_DEPLOYMENT\"]\n",
    "CHAT_API_VERSION = os.environ[\"OPENAI_CHAT_API_VERSION\"]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Completion Quickstart"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<OpenAIObject text_completion id=cmpl-7BKh7Mg0EIr4y32fEomIkBybL1pR4 at 0x1591ca3c5f0> JSON: {\n",
       "  \"choices\": [\n",
       "    {\n",
       "      \"finish_reason\": \"length\",\n",
       "      \"index\": 0,\n",
       "      \"logprobs\": null,\n",
       "      \"text\": \"\\n\\nHi there! How can I help you\"\n",
       "    }\n",
       "  ],\n",
       "  \"created\": 1682935513,\n",
       "  \"id\": \"cmpl-7BKh7Mg0EIr4y32fEomIkBybL1pR4\",\n",
       "  \"model\": \"text-davinci-003\",\n",
       "  \"object\": \"text_completion\",\n",
       "  \"usage\": {\n",
       "    \"completion_tokens\": 10,\n",
       "    \"prompt_tokens\": 2,\n",
       "    \"total_tokens\": 12\n",
       "  }\n",
       "}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import openai\n",
    "\n",
    "response = openai.Completion.create(\n",
    "    engine=COMPLETION_MODEL,\n",
    "    prompt=\"Hello.\",\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mAzureOpenAI\u001b[0m\n",
      "Params: {'deployment_name': 'text-davinci-003', 'model_name': 'text-davinci-003', 'temperature': 0.7, 'max_tokens': 10, 'top_p': 1, 'frequency_penalty': 0, 'presence_penalty': 0, 'n': 1, 'best_of': 1, 'request_timeout': None, 'logit_bias': {}}\n"
     ]
    }
   ],
   "source": [
    "from langchain.llms import AzureOpenAI\n",
    "\n",
    "llm = AzureOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"], # optional, specify if value is not in environment variable OPENAI_API_KEY\n",
    "    model_name=COMPLETION_MODEL,\n",
    "    deployment_name=COMPLETION_DEPLOYMENT,\n",
    "    temperature=0.7,\n",
    "    max_tokens=10\n",
    ")\n",
    "\n",
    "print(llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nQ: What did the fish say when'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm(\"Tell me a joke.\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chat Model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.memory import ConversationSummaryBufferMemory\n",
    "from langchain.schema import (\n",
    "    AIMessage,\n",
    "    HumanMessage,\n",
    "    SystemMessage\n",
    ")\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    deployment_name=CHAT_DEPLOYMENT,\n",
    "    openai_api_version=CHAT_API_VERSION\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='Wah, I really love using Python for programming with Jupyter notebooks leh!', additional_kwargs={})"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are an assistant who likes to respond with dad jokes.\"),\n",
    "    HumanMessage(content=\"Translate this sentense from English to Singlish. I love Python programming with Jupyter notebooks!\")\n",
    "]\n",
    "\n",
    "chat(messages)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Prompt Templates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content='I love Python programming with Jupyter notebooks lah!', additional_kwargs={})"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.prompts.chat import (\n",
    "    ChatPromptTemplate,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate,\n",
    ")\n",
    "\n",
    "chat = AzureChatOpenAI(\n",
    "    deployment_name=CHAT_DEPLOYMENT,\n",
    "    openai_api_version=CHAT_API_VERSION\n",
    ")\n",
    "\n",
    "template=\"You are a helpful assistant that translates {input_language} to {output_language}.\"\n",
    "system_message_prompt = SystemMessagePromptTemplate.from_template(template)\n",
    "human_template=\"{text}\"\n",
    "human_message_prompt = HumanMessagePromptTemplate.from_template(human_template)\n",
    "\n",
    "chat_prompt = ChatPromptTemplate.from_messages([system_message_prompt, human_message_prompt])\n",
    "\n",
    "# get a chat completion from the formatted messages\n",
    "chat(\n",
    "    chat_prompt.format_prompt(\n",
    "        input_language=\"English\",\n",
    "        output_language=\"Singlish\",\n",
    "        text=\"I love Python programming with Jupyter notebooks!\"\n",
    "    )\n",
    "    .to_messages()\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.prompts import (\n",
    "    ChatPromptTemplate,\n",
    "    MessagesPlaceholder,\n",
    "    SystemMessagePromptTemplate,\n",
    "    HumanMessagePromptTemplate\n",
    ")\n",
    "from langchain.chains import ConversationChain\n",
    "from langchain.chat_models import AzureChatOpenAI\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    SystemMessagePromptTemplate.from_template(\"The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to the question, it truthfully says it does not know.\"),\n",
    "    MessagesPlaceholder(variable_name=\"history\"),\n",
    "    HumanMessagePromptTemplate.from_template(\"{input}\")\n",
    "])\n",
    "\n",
    "llm = AzureChatOpenAI(\n",
    "    deployment_name=CHAT_DEPLOYMENT,\n",
    "    openai_api_version=CHAT_API_VERSION\n",
    ")\n",
    "memory = ConversationBufferMemory(return_messages=True)\n",
    "conversation = ConversationChain(memory=memory, prompt=prompt, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation.predict(input=\"Hi there!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"That's great! I am always happy to chat. Is there anything specific you would like to talk about?\""
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"I'm doing well! Just having a conversation with an AI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Ako po ay isang artificial intelligence o AI na nilikha upang magbigay ng tulong sa mga tao sa kanilang mga pangangailangan. Ginagamit ako sa iba't ibang larangan tulad ng pagproseso ng data, pagpapayo sa mga kliyente, at maging sa pagtugon sa simpleng mga tanong tulad ng mga itatanong ninyo. Ang aking kakayahan ay nakabatay sa mga algorithm at kalkulasyon, na nagbibigay sa akin ng kakayahang magbigay ng mga sagot at solusyon sa mga problema.\""
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.predict(input=\"Kwentohan mo ako tungkol sa sarili mo.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory=ConversationBufferMemory(chat_memory=ChatMessageHistory(messages=[HumanMessage(content='Hi there!', additional_kwargs={}), AIMessage(content='Hello! How can I assist you today?', additional_kwargs={}), HumanMessage(content=\"I'm doing well! Just having a conversation with an AI.\", additional_kwargs={}), AIMessage(content=\"That's great! I am always happy to chat. Is there anything specific you would like to talk about?\", additional_kwargs={}), HumanMessage(content='Kwentohan mo ako tungkol sa sarili mo.', additional_kwargs={}), AIMessage(content=\"Ako po ay isang artificial intelligence o AI na nilikha upang magbigay ng tulong sa mga tao sa kanilang mga pangangailangan. Ginagamit ako sa iba't ibang larangan tulad ng pagproseso ng data, pagpapayo sa mga kliyente, at maging sa pagtugon sa simpleng mga tanong tulad ng mga itatanong ninyo. Ang aking kakayahan ay nakabatay sa mga algorithm at kalkulasyon, na nagbibigay sa akin ng kakayahang magbigay ng mga sagot at solusyon sa mga problema.\", additional_kwargs={})]), output_key=None, input_key=None, return_messages=True, human_prefix='Human', ai_prefix='AI', memory_key='history') callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x00000285284C3410> verbose=False prompt=ChatPromptTemplate(input_variables=['input', 'history'], output_parser=None, partial_variables={}, messages=[SystemMessagePromptTemplate(prompt=PromptTemplate(input_variables=[], output_parser=None, partial_variables={}, template='The following is a friendly conversation between a human and an AI. The AI is talkative and provides lots of specific details from its context. If the AI does not know the answer to the question, it truthfully says it does not know.', template_format='f-string', validate_template=True), additional_kwargs={}), MessagesPlaceholder(variable_name='history'), HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['input'], output_parser=None, partial_variables={}, template='{input}', template_format='f-string', validate_template=True), additional_kwargs={})]) llm=AzureChatOpenAI(verbose=False, callback_manager=<langchain.callbacks.shared.SharedCallbackManager object at 0x00000285284C3410>, client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, model_name='gpt-3.5-turbo', temperature=0.7, model_kwargs={}, openai_api_key='', openai_organization='', request_timeout=60, max_retries=6, streaming=False, n=1, max_tokens=None, deployment_name='gpt-35-turbo', openai_api_type='azure', openai_api_base='', openai_api_version='2023-03-15-preview') output_key='response' input_key='input'\n"
     ]
    }
   ],
   "source": [
    "print(conversation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Greetings! How may I assist you today?'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conversation.run(\"Hello there!\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ChatSummaryBufferMemory\n",
    "- The `orchestrator-simple` isn't summarizing the conversation, try [this](https://python.langchain.com/en/latest/modules/memory/types/summary_buffer.html) out first in this notebook."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
