{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set environment variables in the .env file.\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_TYPE = os.environ[\"OPENAI_API_TYPE\"]\n",
    "OPENAI_API_VERSION = os.environ[\"OPENAI_API_VERSION\"]\n",
    "OPENAI_API_BASE = os.environ[\"OPENAI_API_BASE\"]\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "OPENAI_DEPLOYMENT_NAME = os.environ[\"OPENAI_DEPLOYMENT_NAME\"]\n",
    "\n",
    "AZURE_AI_VISION_ENDPOINT = os.environ[\"AZURE_AI_VISION_ENDPOINT\"]\n",
    "AZURE_AI_VISION_KEY = os.environ[\"AZURE_AI_VISION_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Vision Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image from Public Web\n",
    "If the image is available via an image URL, the OpenAI SDK may be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason=None, index=0, message=ChatCompletionMessage(content='The image shows four individuals standing against a purple background. From left to right, the first person is wearing a purple t-shirt with dark pants, the second individual is dressed in a dark purple sweater with black pants, the third person is wearing a denim jacket over a black top paired with black pants, and the fourth person is in a light purple long-sleeve shirt with light pink pants. All four individuals appear to be casually posing for the photo.', role='assistant', function_call=None, tool_calls=None), finish_details={'type': 'stop', 'stop': '<|fim_suffix|>'}, content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = OPENAI_API_BASE, \n",
    "  api_key=OPENAI_API_KEY,  \n",
    "  api_version=OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=OPENAI_DEPLOYMENT_NAME,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"What is in this image?\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"https://media.wired.com/photos/64ed0bc52da6c6d86e70e575/master/w_1280,c_limit/WI100123_FF_OpenAI_01.jpg\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image from Local Machine (REST API)\n",
    "Below is the code sample from OpenAI and Azure OpenAI as of 2024-01-02. It uses the Rest API instead of the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'tuple' object has no attribute 'tb_frame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 54\u001b[0m\n\u001b[0;32m     53\u001b[0m     response \u001b[38;5;241m=\u001b[39m requests\u001b[38;5;241m.\u001b[39mpost(GPT4V_ENDPOINT, headers\u001b[38;5;241m=\u001b[39mheaders, json\u001b[38;5;241m=\u001b[39mpayload)\n\u001b[1;32m---> 54\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Will raise an HTTPError if the HTTP request returned an unsuccessful status code\u001b[39;00m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Python311\\Lib\\site-packages\\requests\\models.py:1021\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1020\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1021\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 404 Client Error: Resource Not Found for url: https://razopenai-sc.openai.azure.com/openai/deployments/gpt4o/extensions/chat/completions?api-version=2024-02-01",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mSystemExit\u001b[0m                                Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "Cell \u001b[1;32mIn[3], line 56\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m requests\u001b[38;5;241m.\u001b[39mRequestException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m---> 56\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSystemExit\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to make the request. Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# Handle the response as needed (e.g., print or process)\u001b[39;00m\n",
      "\u001b[1;31mSystemExit\u001b[0m: Failed to make the request. Error: 404 Client Error: Resource Not Found for url: https://razopenai-sc.openai.azure.com/openai/deployments/gpt4o/extensions/chat/completions?api-version=2024-02-01",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "    \u001b[1;31m[... skipping hidden 1 frame]\u001b[0m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\interactiveshell.py:2092\u001b[0m, in \u001b[0;36mInteractiveShell.showtraceback\u001b[1;34m(self, exc_tuple, filename, tb_offset, exception_only, running_compiled_code)\u001b[0m\n\u001b[0;32m   2089\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m exception_only:\n\u001b[0;32m   2090\u001b[0m     stb \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAn exception has occurred, use \u001b[39m\u001b[38;5;124m%\u001b[39m\u001b[38;5;124mtb to see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m   2091\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mthe full traceback.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m-> 2092\u001b[0m     stb\u001b[38;5;241m.\u001b[39mextend(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInteractiveTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_exception_only\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2093\u001b[0m \u001b[43m                                                     \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   2094\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2095\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   2096\u001b[0m         \u001b[38;5;66;03m# Exception classes can customise their traceback - we\u001b[39;00m\n\u001b[0;32m   2097\u001b[0m         \u001b[38;5;66;03m# use this in IPython.parallel for exceptions occurring\u001b[39;00m\n\u001b[0;32m   2098\u001b[0m         \u001b[38;5;66;03m# in the engines. This should return a list of strings.\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py:644\u001b[0m, in \u001b[0;36mListTB.get_exception_only\u001b[1;34m(self, etype, value)\u001b[0m\n\u001b[0;32m    636\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_exception_only\u001b[39m(\u001b[38;5;28mself\u001b[39m, etype, value):\n\u001b[0;32m    637\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Only print the exception type and message, without a traceback.\u001b[39;00m\n\u001b[0;32m    638\u001b[0m \n\u001b[0;32m    639\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    642\u001b[0m \u001b[38;5;124;03m    value : exception value\u001b[39;00m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 644\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mListTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py:511\u001b[0m, in \u001b[0;36mListTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, context)\u001b[0m\n\u001b[0;32m    508\u001b[0m     chained_exc_ids\u001b[38;5;241m.\u001b[39madd(\u001b[38;5;28mid\u001b[39m(exception[\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m    509\u001b[0m     chained_exceptions_tb_offset \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    510\u001b[0m     out_list \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 511\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    512\u001b[0m \u001b[43m            \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchained_exc_ids\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    513\u001b[0m \u001b[43m            \u001b[49m\u001b[43mchained_exceptions_tb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    514\u001b[0m         \u001b[38;5;241m+\u001b[39m chained_exception_message\n\u001b[0;32m    515\u001b[0m         \u001b[38;5;241m+\u001b[39m out_list)\n\u001b[0;32m    517\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m out_list\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py:1310\u001b[0m, in \u001b[0;36mAutoFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1308\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1309\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtb \u001b[38;5;241m=\u001b[39m tb\n\u001b[1;32m-> 1310\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mFormattedTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1311\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py:1199\u001b[0m, in \u001b[0;36mFormattedTB.structured_traceback\u001b[1;34m(self, etype, value, tb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1196\u001b[0m mode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmode\n\u001b[0;32m   1197\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose_modes:\n\u001b[0;32m   1198\u001b[0m     \u001b[38;5;66;03m# Verbose modes need a full traceback\u001b[39;00m\n\u001b[1;32m-> 1199\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVerboseTB\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstructured_traceback\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1200\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\n\u001b[0;32m   1201\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1202\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMinimal\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m   1203\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m ListTB\u001b[38;5;241m.\u001b[39mget_exception_only(\u001b[38;5;28mself\u001b[39m, etype, value)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py:1052\u001b[0m, in \u001b[0;36mVerboseTB.structured_traceback\u001b[1;34m(self, etype, evalue, etb, tb_offset, number_of_lines_of_context)\u001b[0m\n\u001b[0;32m   1043\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstructured_traceback\u001b[39m(\n\u001b[0;32m   1044\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1045\u001b[0m     etype: \u001b[38;5;28mtype\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1049\u001b[0m     number_of_lines_of_context: \u001b[38;5;28mint\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m5\u001b[39m,\n\u001b[0;32m   1050\u001b[0m ):\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a nice text document describing the traceback.\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1052\u001b[0m     formatted_exception \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mformat_exception_as_a_whole\u001b[49m\u001b[43m(\u001b[49m\u001b[43metype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1053\u001b[0m \u001b[43m                                                           \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1055\u001b[0m     colors \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mColors  \u001b[38;5;66;03m# just a shorthand + quicker name lookup\u001b[39;00m\n\u001b[0;32m   1056\u001b[0m     colorsnormal \u001b[38;5;241m=\u001b[39m colors\u001b[38;5;241m.\u001b[39mNormal  \u001b[38;5;66;03m# used a lot\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py:953\u001b[0m, in \u001b[0;36mVerboseTB.format_exception_as_a_whole\u001b[1;34m(self, etype, evalue, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m    950\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tb_offset, \u001b[38;5;28mint\u001b[39m)\n\u001b[0;32m    951\u001b[0m head \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprepare_header(etype, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlong_header)\n\u001b[0;32m    952\u001b[0m records \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m--> 953\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_records\u001b[49m\u001b[43m(\u001b[49m\u001b[43metb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnumber_of_lines_of_context\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtb_offset\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mif\u001b[39;00m etb \u001b[38;5;28;01melse\u001b[39;00m []\n\u001b[0;32m    954\u001b[0m )\n\u001b[0;32m    956\u001b[0m frames \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    957\u001b[0m skipped \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\IPython\\core\\ultratb.py:1021\u001b[0m, in \u001b[0;36mVerboseTB.get_records\u001b[1;34m(self, etb, number_of_lines_of_context, tb_offset)\u001b[0m\n\u001b[0;32m   1019\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m cf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1020\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1021\u001b[0m         source_file \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetsourcefile(\u001b[43metb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtb_frame\u001b[49m)\n\u001b[0;32m   1022\u001b[0m         lines, first \u001b[38;5;241m=\u001b[39m inspect\u001b[38;5;241m.\u001b[39mgetsourcelines(etb\u001b[38;5;241m.\u001b[39mtb_frame)\n\u001b[0;32m   1023\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'tb_frame'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "# Configuration\n",
    "IMAGE_PATH = \"../sampledata/image-barbeque.png\"\n",
    "encoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": OPENAI_API_KEY,\n",
    "}\n",
    "\n",
    "system_prompt = \"\"\"You are an assistant helps the blind. In addition to answering questions, you help the blind understand what is in the images provided by the user.\n",
    "\n",
    "Image outputs should include:\n",
    "- Detailed description\n",
    "- Suggested tags\n",
    "- Key-value pairs (if the image is a form, receipt, invoice, etc.)\"\"\"\n",
    "\n",
    "# Payload for the request\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": system_prompt\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "GPT4V_ENDPOINT = f\"{OPENAI_API_BASE}/openai/deployments/{OPENAI_DEPLOYMENT_NAME}/extensions/chat/completions?api-version={OPENAI_API_VERSION}\"\n",
    "\n",
    "# Send request\n",
    "try:\n",
    "    response = requests.post(GPT4V_ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "# Handle the response as needed (e.g., print or process)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image from Local Machine (SDK)\n",
    "Below is working code using the OpenAI SDK. This code is not in the official samples as of 2024-01-02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'Choice' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 48\u001b[0m\n\u001b[0;32m     14\u001b[0m system_prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mYou are an assistant helps the blind. In addition to answering questions, you help the blind understand what is in the images provided by the user.\u001b[39m\n\u001b[0;32m     15\u001b[0m \n\u001b[0;32m     16\u001b[0m \u001b[38;5;124mImage outputs should include:\u001b[39m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;124m- Detailed description\u001b[39m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;124m- Suggested tags\u001b[39m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124m- Key-value pairs (if the image is a form, receipt, invoice, etc.)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[0;32m     21\u001b[0m response \u001b[38;5;241m=\u001b[39m client\u001b[38;5;241m.\u001b[39mchat\u001b[38;5;241m.\u001b[39mcompletions\u001b[38;5;241m.\u001b[39mcreate(\n\u001b[0;32m     22\u001b[0m   model\u001b[38;5;241m=\u001b[39mOPENAI_DEPLOYMENT_NAME,\n\u001b[0;32m     23\u001b[0m   messages\u001b[38;5;241m=\u001b[39m[\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     45\u001b[0m   max_tokens\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m300\u001b[39m,\n\u001b[0;32m     46\u001b[0m )\n\u001b[1;32m---> 48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchoices\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmessage\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "\u001b[1;31mTypeError\u001b[0m: 'Choice' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "import base64\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = OPENAI_API_BASE, \n",
    "  api_key=OPENAI_API_KEY,  \n",
    "  api_version=OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "IMAGE_PATH = \"../sampledata/image-barbeque.png\"\n",
    "encoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')\n",
    "\n",
    "system_prompt = \"\"\"You are an assistant helps the blind. In addition to answering questions, you help the blind understand what is in the images provided by the user.\n",
    "\n",
    "Image outputs should include:\n",
    "- Detailed description\n",
    "- Suggested tags\n",
    "- Key-value pairs (if the image is a form, receipt, invoice, etc.)\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=OPENAI_DEPLOYMENT_NAME,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": system_prompt\n",
    "        }\n",
    "      ],\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0]['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Vision + Azure AI Vision (REST API)\n",
    "To use Azure AI Vision, Azure OpenAI REST API is **required**. This is because it is using the Chat Completions _Extensions_ API, which the official SDK doesn't have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8cUbceCQC3SQ6WNS2vxA1jfe5x6cL', 'object': 'chat.completion', 'created': 1704184804, 'model': 'gpt-4', 'choices': [{'finish_details': {'type': 'stop', 'stop': '<|fim_suffix|>'}, 'index': 0, 'message': {'role': 'assistant', 'content': \"Description:\\nThe image captures an outdoor setting with a man standing in the foreground, actively grilling chicken on a small charcoal grill. The man is wearing a white t-shirt and a bright yellow apron. His face is blurred for privacy. He appears to be holding a pair of tongs in one hand and a piece of bread or bun in the other. In the background, two children are playing; one is playing with a hula hoop, and the other is standing nearby, possibly waiting for her turn. There's a red soccer ball on the ground beside them. The environment is lush with green grass and trees, suggesting a park or a backyard garden. A pile of dirt or sand and a few scattered fallen branches are also visible in the background.\\n\\nSuggested Tags:\\nOutdoor, Family, BBQ, Grilling, Children, Playing, Park, Garden, Casual, Weekend, Daytime, Recreation, Leisure, Nature.\\n\\nNote:\\nAs requested, the description excludes the blurred face, focusing on the activities and surroundings.\"}, 'enhancements': {'grounding': {'lines': [{'text': \"Description:\\nThe image captures an outdoor setting with a man standing in the foreground, actively grilling chicken on a small charcoal grill. The man is wearing a white t-shirt and a bright yellow apron. His face is blurred for privacy. He appears to be holding a pair of tongs in one hand and a piece of bread or bun in the other. In the background, two children are playing; one is playing with a hula hoop, and the other is standing nearby, possibly waiting for her turn. There's a red soccer ball on the ground beside them. The environment is lush with green grass and trees, suggesting a park or a backyard garden. A pile of dirt or sand and a few scattered fallen branches are also visible in the background.\\n\\nSuggested Tags:\\nOutdoor, Family, BBQ, Grilling, Children, Playing, Park, Garden, Casual, Weekend, Daytime, Recreation, Leisure, Nature.\\n\\nNote:\\nAs requested, the description excludes the blurred face, focusing on the activities and surroundings.\", 'spans': [{'text': 'the man', 'length': 7, 'offset': 143, 'polygon': [{'x': 0.25550001859664917, 'y': 0.15150000154972076}, {'x': 0.5385000705718994, 'y': 0.15150000154972076}, {'x': 0.5385000705718994, 'y': 0.997499942779541}, {'x': 0.25550001859664917, 'y': 0.997499942779541}]}, {'text': 'a white t-shirt', 'length': 15, 'offset': 162, 'polygon': [{'x': 0.26850003004074097, 'y': 0.2944999933242798}, {'x': 0.48450005054473877, 'y': 0.2944999933242798}, {'x': 0.48450005054473877, 'y': 0.5484999418258667}, {'x': 0.26850003004074097, 'y': 0.5484999418258667}]}, {'text': 'a bright yellow apron', 'length': 21, 'offset': 182, 'polygon': [{'x': 0.32249999046325684, 'y': 0.2954999804496765}, {'x': 0.49250003695487976, 'y': 0.2954999804496765}, {'x': 0.49250003695487976, 'y': 0.8374999761581421}, {'x': 0.32249999046325684, 'y': 0.8374999761581421}]}]}], 'status': 'Success'}}}], 'usage': {'prompt_tokens': 1197, 'completion_tokens': 205, 'total_tokens': 1402}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "# Configuration\n",
    "IMAGE_PATH = \"../sampledata/image-barbeque.png\"\n",
    "encoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": OPENAI_API_KEY,\n",
    "}\n",
    "\n",
    "system_prompt = \"\"\"You are an assistant helps the blind. In addition to answering questions, you help the blind understand what is in the images provided by the user.\n",
    "\n",
    "Image outputs should include:\n",
    "- Detailed description\n",
    "- Suggested tags\n",
    "- Key-value pairs (if the image is a form, receipt, invoice, etc.)\"\"\"\n",
    "\n",
    "# Payload for the request\n",
    "payload = {\n",
    "  \"enhancements\": {\n",
    "    \"ocr\": {\n",
    "      \"enabled\": True\n",
    "    },\n",
    "    \"grounding\": {\n",
    "      \"enabled\": True\n",
    "    }\n",
    "  },\n",
    "  \"dataSources\": [\n",
    "    {\n",
    "      \"type\" : \"AzureComputerVision\",\n",
    "      \"parameters\" : {\n",
    "        \"endpoint\" : AZURE_AI_VISION_ENDPOINT,\n",
    "        \"key\" : AZURE_AI_VISION_KEY\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": system_prompt\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "GPT4V_ENDPOINT = f\"{OPENAI_API_BASE}/openai/deployments/{OPENAI_DEPLOYMENT_NAME}/extensions/chat/completions?api-version={OPENAI_API_VERSION}\"\n",
    "\n",
    "# Send request\n",
    "try:\n",
    "    response = requests.post(GPT4V_ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "# Handle the response as needed (e.g., print or process)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 {\"name\":\"visionindex01\",\"userData\":{},\"features\":[{\"name\":\"vision\",\"modelVersion\":\"2023-05-31\",\"domain\":\"surveillance\"}],\"eTag\":\"\\\"2c98b25f876a41f0a181a675c5945420\\\"\",\"createdDateTime\":\"2024-01-11T05:58:39.2433630Z\",\"lastModifiedDateTime\":\"2024-01-11T05:58:39.2433630Z\"}\n",
      "202 {\"name\":\"my-ingestion\",\"state\":\"Running\",\"batchName\":\"91bdc888-ced8-4164-8f66-bda91cb14741\",\"createdDateTime\":\"2024-01-11T05:58:40.6027401Z\",\"lastModifiedDateTime\":\"2024-01-11T05:58:40.9933680Z\"}\n",
      "{'value': [{'name': 'my-ingestion', 'state': 'Completed', 'batchName': '91bdc888-ced8-4164-8f66-bda91cb14741', 'createdDateTime': '2024-01-11T05:58:40.6027401Z', 'lastModifiedDateTime': '2024-01-11T05:59:05.8527613Z'}]}\n",
      "Ingestion completed.\n",
      "{'id': 'chatcmpl-8fiOEhCPexw0WJvLYpN3HDRm20ZfK', 'object': 'chat.completion', 'created': 1704952774, 'model': 'gpt-4', 'choices': [{'finish_details': {'type': 'stop', 'stop': '<|fim_suffix|>'}, 'index': 0, 'message': {'role': 'assistant', 'content': 'The images provided are from various timestamps of a video that appears to be set in an industrial or manufacturing environment. \\n\\n1. Timestamp 00:00:02 - The image shows a wide view of a manufacturing workshop with various machines, such as CNC machines and lathes, aligned in rows. There are no people visible in the frame.\\n\\n2. Timestamp 00:00:16 - A worker wearing a high-visibility vest and a hard hat is visible walking through the aisle between the machinery.\\n\\n3. Timestamp 00:00:27 to 00:00:33 - These images show a different angle of the workshop with a worker pushing a manual pallet jack across the room.\\n\\n4. Timestamp 00:01:10 to 00:02:02 - A series of images showing a different section of the workshop, this time with workers interacting with CNC machines and work benches with various tools and metal parts. One worker is seen inspecting a piece while another is walking down the aisle.\\n\\n5. Timestamp 00:02:08 - A worker is seen sitting on a chair, seemingly taking a break from work.\\n\\n6. Timestamp 00:02:15 - The scene shifts to an outdoor area showing a loading dock with dumpsters, a parked truck, and a worker walking in the distance.\\n\\n7. Timestamp 00:02:28 to 00:03:03 - Back inside, we see a section of the workshop with a worker wearing a high-visibility vest working near a bench with blueprints and another one carrying a box.\\n\\n8. Timestamp 00:03:07 - The scene shows a worker in a different part of the workshop with various tools and workbenches.\\n\\n9. Timestamp 00:03:23 to 00:03:24 - The final images return to the outdoor loading dock area, now with a forklift in operation and workers around.\\n\\nSuggested tags for these images include: manufacturing, workshop, machinery, workers, industrial, CNC machines, tools, workbenches, loading dock, forklift, safety gear, high-visibility vest, hard hat.\\n\\nThese images do not appear to contain any forms, receipts, or invoices that would require key-value pair extraction.'}}], 'usage': {'prompt_tokens': 2039, 'completion_tokens': 451, 'total_tokens': 2490}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Configuration\n",
    "GPT4V_ENDPOINT = f\"{OPENAI_API_BASE}/openai/deployments/{OPENAI_DEPLOYMENT_NAME}/extensions/chat/completions?api-version={OPENAI_API_VERSION}\"\n",
    "\n",
    "## ingest the video\n",
    "VIDEO_FILE_SAS_URL = \"https://raztypestore.blob.core.windows.net/temp/video-factory.mp4?sv=2023-01-03&st=2024-01-11T05%3A42%3A07Z&se=2024-03-30T05%3A42%3A00Z&sr=b&sp=r&sig=sArMtPI4ICs95MQrhvYGP5oAFB8DjpjWtJNwEgqPzYE%3D\"\n",
    "VIDEO_INDEX_NAME = \"visionindex01\" # this needs to be unique, append number. To delete old indices, use the REST API https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/reference-video-search\n",
    "VIDEO_DOCUMENT_ID = \"AOAIChatDocument\"\n",
    "\n",
    "def create_video_index(vision_api_endpoint, vision_api_key, index_name):\n",
    "    url = f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}?api-version=2023-05-01-preview\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key, \"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"features\": [\n",
    "            {\"name\": \"vision\", \"domain\": \"surveillance\"}\n",
    "        ]\n",
    "    }\n",
    "    response = requests.put(url, headers=headers, json=data)\n",
    "    return response\n",
    "\n",
    "def add_video_to_index(vision_api_endpoint, vision_api_key, index_name, video_url, video_id):\n",
    "    url = f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}/ingestions/my-ingestion?api-version=2023-05-01-preview\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key, \"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        'videos': [{'mode': 'add', 'documentId': video_id, 'documentUrl': video_url}]\n",
    "    }\n",
    "    response = requests.put(url, headers=headers, json=data)\n",
    "    return response\n",
    "\n",
    "def wait_for_ingestion_completion(vision_api_endpoint, vision_api_key, index_name, max_retries=30):\n",
    "    url = f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}/ingestions?api-version=2023-05-01-preview\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key}\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        time.sleep(10)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            state_data = response.json()\n",
    "            if state_data['value'][0]['state'] == 'Completed':\n",
    "                print(state_data)\n",
    "                print('Ingestion completed.')\n",
    "                return True\n",
    "            elif state_data['value'][0]['state'] == 'Failed':\n",
    "                print(state_data)\n",
    "                print('Ingestion failed.')\n",
    "                return False\n",
    "        retries += 1\n",
    "    return False\n",
    "\n",
    "\n",
    "# Step 1: Create an Index\n",
    "response = create_video_index(AZURE_AI_VISION_ENDPOINT, AZURE_AI_VISION_KEY, VIDEO_INDEX_NAME)\n",
    "print(response.status_code, response.text)\n",
    "\n",
    "# Step 2: Add a video file to the index\n",
    "response = add_video_to_index(AZURE_AI_VISION_ENDPOINT, AZURE_AI_VISION_KEY, VIDEO_INDEX_NAME, VIDEO_FILE_SAS_URL, VIDEO_DOCUMENT_ID)\n",
    "print(response.status_code, response.text)\n",
    "\n",
    "# Step 3: Wait for ingestion to complete\n",
    "if not wait_for_ingestion_completion(AZURE_AI_VISION_ENDPOINT, AZURE_AI_VISION_KEY, VIDEO_INDEX_NAME):\n",
    "    print(\"Ingestion did not complete within the expected time.\")\n",
    "\n",
    "\n",
    "## Chat with GPT-4V\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": OPENAI_API_KEY,\n",
    "}\n",
    "\n",
    "system_prompt = \"\"\"You are an assistant helps the blind. In addition to answering questions, you help the blind understand what is in the images provided by the user.\n",
    "\n",
    "Image outputs should include:\n",
    "- Detailed description\n",
    "- Suggested tags\n",
    "- Key-value pairs (if the image is a form, receipt, invoice, etc.)\"\"\"\n",
    "\n",
    "# Payload for the request\n",
    "payload = {\n",
    "    \"dataSources\": [\n",
    "        {\n",
    "            \"type\": \"AzureComputerVisionVideoIndex\",\n",
    "            \"parameters\": {\n",
    "                \"computerVisionBaseUrl\": f\"{AZURE_AI_VISION_ENDPOINT}/computervision\",\n",
    "                \"computerVisionApiKey\": AZURE_AI_VISION_KEY,\n",
    "                \"indexName\": VIDEO_INDEX_NAME,\n",
    "                \"videoUrls\": [VIDEO_FILE_SAS_URL]\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"enhancements\": {\n",
    "        \"video\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    },\n",
    "    \"messages\": [\n",
    "     {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": [\n",
    "               {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": system_prompt\n",
    "               }\n",
    "          ]\n",
    "     },\n",
    "     {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "               {\n",
    "                    \"type\": \"acv_document_id\",\n",
    "                    \"acv_document_id\": VIDEO_DOCUMENT_ID\n",
    "               },\n",
    "               {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \" \"\n",
    "               }\n",
    "          ]\n",
    "     }\n",
    "],\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "# Send request\n",
    "try:\n",
    "    response = requests.post(GPT4V_ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "# Handle the response as needed (e.g., print or process)\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
