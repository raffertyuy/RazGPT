{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set environment variables in the .env file.\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_TYPE = os.environ[\"OPENAI_API_TYPE\"]\n",
    "OPENAI_API_VERSION = os.environ[\"OPENAI_API_VERSION\"]\n",
    "OPENAI_API_BASE = os.environ[\"OPENAI_API_BASE\"]\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "OPENAI_DEPLOYMENT_NAME = os.environ[\"OPENAI_DEPLOYMENT_NAME\"]\n",
    "\n",
    "AZURE_AI_VISION_ENDPOINT = os.environ[\"AZURE_AI_VISION_ENDPOINT\"]\n",
    "AZURE_AI_VISION_KEY = os.environ[\"AZURE_AI_VISION_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Vision Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image from Public Web\n",
    "If the image is available via an image URL, the OpenAI SDK may be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason=None, index=0, message=ChatCompletionMessage(content='The image shows four individuals standing against a purple background. From left to right, the first person is wearing a purple t-shirt with dark pants, the second individual is dressed in a dark purple sweater with black pants, the third person is wearing a denim jacket over a black top paired with black pants, and the fourth person is in a light purple long-sleeve shirt with light pink pants. All four individuals appear to be casually posing for the photo.', role='assistant', function_call=None, tool_calls=None), finish_details={'type': 'stop', 'stop': '<|fim_suffix|>'}, content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = OPENAI_API_BASE, \n",
    "  api_key=OPENAI_API_KEY,  \n",
    "  api_version=OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=OPENAI_DEPLOYMENT_NAME,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"What is in this image?\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"https://media.wired.com/photos/64ed0bc52da6c6d86e70e575/master/w_1280,c_limit/WI100123_FF_OpenAI_01.jpg\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image from Local Machine (REST API)\n",
    "Below is the code sample from OpenAI and Azure OpenAI as of 2024-01-02. It uses the Rest API instead of the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8cUYbQWlME7BVEbiyWiZRKkfUCKPr', 'object': 'chat.completion', 'created': 1704184617, 'model': 'gpt-4', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'choices': [{'finish_details': {'type': 'stop', 'stop': '<|fim_suffix|>'}, 'index': 0, 'message': {'role': 'assistant', 'content': 'This is an outdoor image showing a family scene. A person, whose face is blurred, is wearing a white t-shirt and a bright yellow apron, standing in front of a charcoal grill with what appears to be chicken on it. The individual is holding a pair of tongs in one hand and a piece of bread or bun in the other. In the background, two children are playing; one is twirling a hula hoop around her arm while the other appears to be watching. There is a red and white soccer ball on the grass near them. The environment is green with several trees around, suggesting a park or a backyard garden setting.\\n\\nSuggested Tags: Family, Barbecue, Outdoor, Children Playing, Grilling, Park, Picnic, Summer, Family Time\\n\\nPlease note that the faces of the individuals are blurred for privacy.'}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'usage': {'prompt_tokens': 1197, 'completion_tokens': 170, 'total_tokens': 1367}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "# Configuration\n",
    "IMAGE_PATH = \"../sampledata/image-barbeque.png\"\n",
    "encoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": OPENAI_API_KEY,\n",
    "}\n",
    "\n",
    "system_prompt = \"\"\"You are an assistant helps the blind. In addition to answering questions, you help the blind understand what is in the images provided by the user.\n",
    "\n",
    "Image outputs should include:\n",
    "- Detailed description\n",
    "- Suggested tags\n",
    "- Key-value pairs (if the image is a form, receipt, invoice, etc.)\"\"\"\n",
    "\n",
    "# Payload for the request\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": system_prompt\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "GPT4V_ENDPOINT = f\"{OPENAI_API_BASE}/openai/deployments/{OPENAI_DEPLOYMENT_NAME}/extensions/chat/completions?api-version={OPENAI_API_VERSION}\"\n",
    "\n",
    "# Send request\n",
    "try:\n",
    "    response = requests.post(GPT4V_ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "# Handle the response as needed (e.g., print or process)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image from Local Machine (SDK)\n",
    "Below is working code using the OpenAI SDK. This code is not in the official samples as of 2024-01-02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason=None, index=0, message=ChatCompletionMessage(content=\"Description:\\nThe image shows a lively outdoor scene where a man is standing and grilling chicken on a barbecue grill. He is wearing a white T-shirt and a bright yellow apron and is holding barbecue tongs in one hand with what appears to be several pieces of chicken on the grill. His face is blurred for privacy. In the background, there are two children playing with hula hoops in a grassy area with trees around. The child closer to the man is hula hooping, while the younger one is holding a hula hoop and watching, with her face also blurred. Close to the grilling area, there is a deflated red and black soccer ball on the ground.\\n\\nSuggested Tags:\\nOutdoor, Family, BBQ, Grilling, Cooking, Children, Playtime, Recreation, Trees, Nature, Leisure, Activity, Food.\\n\\nPlease note that faces have been intentionally blurred to protect the individuals' privacy.\", role='assistant', function_call=None, tool_calls=None), finish_details={'type': 'stop', 'stop': '<|fim_suffix|>'}, content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "import base64\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = OPENAI_API_BASE, \n",
    "  api_key=OPENAI_API_KEY,  \n",
    "  api_version=OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "IMAGE_PATH = \"../sampledata/image-barbeque.png\"\n",
    "encoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')\n",
    "\n",
    "system_prompt = \"\"\"You are an assistant helps the blind. In addition to answering questions, you help the blind understand what is in the images provided by the user.\n",
    "\n",
    "Image outputs should include:\n",
    "- Detailed description\n",
    "- Suggested tags\n",
    "- Key-value pairs (if the image is a form, receipt, invoice, etc.)\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=OPENAI_DEPLOYMENT_NAME,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": system_prompt\n",
    "        }\n",
    "      ],\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Vision + Azure AI Vision (REST API)\n",
    "To use Azure AI Vision, Azure OpenAI REST API is **required**. This is because it is using the Chat Completions _Extensions_ API, which the official SDK doesn't have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8cUbceCQC3SQ6WNS2vxA1jfe5x6cL', 'object': 'chat.completion', 'created': 1704184804, 'model': 'gpt-4', 'choices': [{'finish_details': {'type': 'stop', 'stop': '<|fim_suffix|>'}, 'index': 0, 'message': {'role': 'assistant', 'content': \"Description:\\nThe image captures an outdoor setting with a man standing in the foreground, actively grilling chicken on a small charcoal grill. The man is wearing a white t-shirt and a bright yellow apron. His face is blurred for privacy. He appears to be holding a pair of tongs in one hand and a piece of bread or bun in the other. In the background, two children are playing; one is playing with a hula hoop, and the other is standing nearby, possibly waiting for her turn. There's a red soccer ball on the ground beside them. The environment is lush with green grass and trees, suggesting a park or a backyard garden. A pile of dirt or sand and a few scattered fallen branches are also visible in the background.\\n\\nSuggested Tags:\\nOutdoor, Family, BBQ, Grilling, Children, Playing, Park, Garden, Casual, Weekend, Daytime, Recreation, Leisure, Nature.\\n\\nNote:\\nAs requested, the description excludes the blurred face, focusing on the activities and surroundings.\"}, 'enhancements': {'grounding': {'lines': [{'text': \"Description:\\nThe image captures an outdoor setting with a man standing in the foreground, actively grilling chicken on a small charcoal grill. The man is wearing a white t-shirt and a bright yellow apron. His face is blurred for privacy. He appears to be holding a pair of tongs in one hand and a piece of bread or bun in the other. In the background, two children are playing; one is playing with a hula hoop, and the other is standing nearby, possibly waiting for her turn. There's a red soccer ball on the ground beside them. The environment is lush with green grass and trees, suggesting a park or a backyard garden. A pile of dirt or sand and a few scattered fallen branches are also visible in the background.\\n\\nSuggested Tags:\\nOutdoor, Family, BBQ, Grilling, Children, Playing, Park, Garden, Casual, Weekend, Daytime, Recreation, Leisure, Nature.\\n\\nNote:\\nAs requested, the description excludes the blurred face, focusing on the activities and surroundings.\", 'spans': [{'text': 'the man', 'length': 7, 'offset': 143, 'polygon': [{'x': 0.25550001859664917, 'y': 0.15150000154972076}, {'x': 0.5385000705718994, 'y': 0.15150000154972076}, {'x': 0.5385000705718994, 'y': 0.997499942779541}, {'x': 0.25550001859664917, 'y': 0.997499942779541}]}, {'text': 'a white t-shirt', 'length': 15, 'offset': 162, 'polygon': [{'x': 0.26850003004074097, 'y': 0.2944999933242798}, {'x': 0.48450005054473877, 'y': 0.2944999933242798}, {'x': 0.48450005054473877, 'y': 0.5484999418258667}, {'x': 0.26850003004074097, 'y': 0.5484999418258667}]}, {'text': 'a bright yellow apron', 'length': 21, 'offset': 182, 'polygon': [{'x': 0.32249999046325684, 'y': 0.2954999804496765}, {'x': 0.49250003695487976, 'y': 0.2954999804496765}, {'x': 0.49250003695487976, 'y': 0.8374999761581421}, {'x': 0.32249999046325684, 'y': 0.8374999761581421}]}]}], 'status': 'Success'}}}], 'usage': {'prompt_tokens': 1197, 'completion_tokens': 205, 'total_tokens': 1402}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "# Configuration\n",
    "IMAGE_PATH = \"../sampledata/image-barbeque.png\"\n",
    "encoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": OPENAI_API_KEY,\n",
    "}\n",
    "\n",
    "system_prompt = \"\"\"You are an assistant helps the blind. In addition to answering questions, you help the blind understand what is in the images provided by the user.\n",
    "\n",
    "Image outputs should include:\n",
    "- Detailed description\n",
    "- Suggested tags\n",
    "- Key-value pairs (if the image is a form, receipt, invoice, etc.)\"\"\"\n",
    "\n",
    "# Payload for the request\n",
    "payload = {\n",
    "  \"enhancements\": {\n",
    "    \"ocr\": {\n",
    "      \"enabled\": True\n",
    "    },\n",
    "    \"grounding\": {\n",
    "      \"enabled\": True\n",
    "    }\n",
    "  },\n",
    "  \"dataSources\": [\n",
    "    {\n",
    "      \"type\" : \"AzureComputerVision\",\n",
    "      \"parameters\" : {\n",
    "        \"endpoint\" : AZURE_AI_VISION_ENDPOINT,\n",
    "        \"key\" : AZURE_AI_VISION_KEY\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": system_prompt\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "GPT4V_ENDPOINT = f\"{OPENAI_API_BASE}/openai/deployments/{OPENAI_DEPLOYMENT_NAME}/extensions/chat/completions?api-version={OPENAI_API_VERSION}\"\n",
    "\n",
    "# Send request\n",
    "try:\n",
    "    response = requests.post(GPT4V_ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "# Handle the response as needed (e.g., print or process)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 {\"name\":\"razgpt4visionvideoindex8\",\"userData\":{},\"features\":[{\"name\":\"vision\",\"modelVersion\":\"2023-05-31\",\"domain\":\"surveillance\"}],\"eTag\":\"\\\"2023c2a0a59e4e9f858692c726a94887\\\"\",\"createdDateTime\":\"2024-01-02T09:51:26.1308243Z\",\"lastModifiedDateTime\":\"2024-01-02T09:51:26.1308243Z\"}\n",
      "202 {\"name\":\"my-ingestion\",\"state\":\"Running\",\"batchName\":\"9e5f0b9f-fa60-4269-b5cf-722d5031c540\",\"createdDateTime\":\"2024-01-02T09:51:27.2089472Z\",\"lastModifiedDateTime\":\"2024-01-02T09:51:27.4120723Z\"}\n",
      "{'value': [{'name': 'my-ingestion', 'state': 'Completed', 'batchName': '9e5f0b9f-fa60-4269-b5cf-722d5031c540', 'createdDateTime': '2024-01-02T09:51:27.2089472Z', 'lastModifiedDateTime': '2024-01-02T09:51:48.4121019Z'}]}\n",
      "Ingestion completed.\n",
      "{'id': 'chatcmpl-8cVjMGc7O5WsYdLMiSNdwie19I9dq', 'object': 'chat.completion', 'created': 1704189128, 'model': 'gpt-4', 'choices': [{'finish_details': {'type': 'stop', 'stop': '<|fim_suffix|>'}, 'index': 0, 'message': {'role': 'assistant', 'content': 'The images provided are sequential frames from a video, each with a specific timestamp.\\n\\n1. Timestamp: 00:00:00 - A close-up of a person\\'s hands tying the laces of a black shoe.\\n2. Timestamp: 00:00:04.0040000 - A dimly lit scene where a person’s hand is reaching out to pick up keys from a white surface.\\n3. Timestamp: 00:00:08.0080000 - A close-up shot of a white car’s side mirror, the reflection and details of the car are not clear.\\n4. Timestamp: 00:00:10.0100000 - A person holding a black smartphone with their left hand, the phone\\'s screen is not visible.\\n5. Timestamp: 00:00:18.0180000 - A blurry motion shot of an office with multiple desks, monitors, and a casual work environment.\\n6. Timestamp: 00:00:24.0240000 - A man in a blue shirt identified as Joe Sullivan, Chief Security Officer at Uber. The name and title are visible in the text overlay.\\n7. Timestamp: 00:00:26.0260000 - A woman standing on a city sidewalk, looking at her phone, with a blurred face, surrounded by other pedestrians and parked bikes.\\n8. Timestamp: 00:00:35.0020000 - A man working at a computer with dual monitors displaying design mockups of mobile applications.\\n9. Timestamp: 00:00:40.0070000 - A street scene with cars, including a prominent black SUV, a traffic signal, and San Francisco’s distinctive architecture and hills in the background.\\n10. Timestamp: 00:00:46.0130000 - A person with a blurred face sitting in the driver\\'s seat of a car, with another individual visible through the car\\'s window.\\n11. Timestamp: 00:00:50.0170000 - A hand interacting with a smartphone mounted in a car, with a map application open showing a route and fare information.\\n12. Timestamp: 00:00:54.0210000 - A close-up of a smartphone with the text \"Identity verified\" on the screen.\\n13. Timestamp: 00:00:59.0260000 - A close-up of a smartphone showing a map and a message regarding an Uber fare of $10.58.\\n14. Timestamp: 00:01:12.0050000 - A man with a blurred face, wearing a blue striped shirt, standing in an office-like setting with soft lighting.\\n15. Timestamp: 00:01:13.0060000 - A smartphone displaying a message about verification received and instructing to check back shortly.\\n16. Timestamp: 00:01:21.0140000 - A close-up of a smartphone screen asking the user to please make a face that fits within an oval shape for verification.\\n17. Timestamp: 00:01:32.0250000 - A profile view of a person with a blurred face, sitting in the passenger seat of a car, looking outside the window.\\n18. Timestamp: 00:01:34.0270000 - A night scene showing a white car driving on a city street with lights and buildings in the background.\\n19. Timestamp: 00:01:39.0320000 - A daytime view of the Golden Gate Bridge with a clear sky, and people and parked cars in the foreground.\\n20. Timestamp: 00:01:46.0060000 - The logo of Microsoft on a white background.\\n\\nSuggested tags: shoes, keys, car, smartphone, office, security officer, city life, computer work, San Francisco, driving, map application, identity verification, ride fare, profile, Golden Gate Bridge, Microsoft logo.'}}], 'usage': {'prompt_tokens': 2146, 'completion_tokens': 784, 'total_tokens': 2930}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Configuration\n",
    "GPT4V_ENDPOINT = f\"{OPENAI_API_BASE}/openai/deployments/{OPENAI_DEPLOYMENT_NAME}/extensions/chat/completions?api-version={OPENAI_API_VERSION}\"\n",
    "\n",
    "## ingest the video\n",
    "VIDEO_FILE_SAS_URL = \"https://raztypestore.blob.core.windows.net/temp/UberCognitiveServices.mp4?sv=2023-01-03&st=2024-01-02T08%3A35%3A43Z&se=2025-01-03T08%3A35%3A00Z&sr=b&sp=r&sig=7WRhu0YdOQAxJ7Uj8OKKBeZtPn6m4RhkqXCz03RP7IY%3D\"\n",
    "VIDEO_INDEX_NAME = \"RazGpt4VisionVideoIndex8\" # this needs to be unique, append number. To delete old indices, use the REST API https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/reference-video-search\n",
    "VIDEO_DOCUMENT_ID = \"AOAIChatDocument\"\n",
    "\n",
    "def create_video_index(vision_api_endpoint, vision_api_key, index_name):\n",
    "    url = f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}?api-version=2023-05-01-preview\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key, \"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"features\": [\n",
    "            {\"name\": \"vision\", \"domain\": \"surveillance\"}\n",
    "        ]\n",
    "    }\n",
    "    response = requests.put(url, headers=headers, json=data)\n",
    "    return response\n",
    "\n",
    "def add_video_to_index(vision_api_endpoint, vision_api_key, index_name, video_url, video_id):\n",
    "    url = f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}/ingestions/my-ingestion?api-version=2023-05-01-preview\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key, \"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        'videos': [{'mode': 'add', 'documentId': video_id, 'documentUrl': video_url}]\n",
    "    }\n",
    "    response = requests.put(url, headers=headers, json=data)\n",
    "    return response\n",
    "\n",
    "def wait_for_ingestion_completion(vision_api_endpoint, vision_api_key, index_name, max_retries=30):\n",
    "    url = f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}/ingestions?api-version=2023-05-01-preview\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key}\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        time.sleep(10)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            state_data = response.json()\n",
    "            if state_data['value'][0]['state'] == 'Completed':\n",
    "                print(state_data)\n",
    "                print('Ingestion completed.')\n",
    "                return True\n",
    "            elif state_data['value'][0]['state'] == 'Failed':\n",
    "                print(state_data)\n",
    "                print('Ingestion failed.')\n",
    "                return False\n",
    "        retries += 1\n",
    "    return False\n",
    "\n",
    "\n",
    "# Step 1: Create an Index\n",
    "response = create_video_index(AZURE_AI_VISION_ENDPOINT, AZURE_AI_VISION_KEY, VIDEO_INDEX_NAME)\n",
    "print(response.status_code, response.text)\n",
    "\n",
    "# Step 2: Add a video file to the index\n",
    "response = add_video_to_index(AZURE_AI_VISION_ENDPOINT, AZURE_AI_VISION_KEY, VIDEO_INDEX_NAME, VIDEO_FILE_SAS_URL, VIDEO_DOCUMENT_ID)\n",
    "print(response.status_code, response.text)\n",
    "\n",
    "# Step 3: Wait for ingestion to complete\n",
    "if not wait_for_ingestion_completion(AZURE_AI_VISION_ENDPOINT, AZURE_AI_VISION_KEY, VIDEO_INDEX_NAME):\n",
    "    print(\"Ingestion did not complete within the expected time.\")\n",
    "\n",
    "\n",
    "## Chat with GPT-4V\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": OPENAI_API_KEY,\n",
    "}\n",
    "\n",
    "system_prompt = \"\"\"You are an assistant helps the blind. In addition to answering questions, you help the blind understand what is in the images provided by the user.\n",
    "\n",
    "Image outputs should include:\n",
    "- Detailed description\n",
    "- Suggested tags\n",
    "- Key-value pairs (if the image is a form, receipt, invoice, etc.)\"\"\"\n",
    "\n",
    "# Payload for the request\n",
    "payload = {\n",
    "    \"dataSources\": [\n",
    "        {\n",
    "            \"type\": \"AzureComputerVisionVideoIndex\",\n",
    "            \"parameters\": {\n",
    "                \"computerVisionBaseUrl\": f\"{AZURE_AI_VISION_ENDPOINT}/computervision\",\n",
    "                \"computerVisionApiKey\": AZURE_AI_VISION_KEY,\n",
    "                \"indexName\": VIDEO_INDEX_NAME,\n",
    "                \"videoUrls\": [VIDEO_FILE_SAS_URL]\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"enhancements\": {\n",
    "        \"video\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    },\n",
    "    \"messages\": [\n",
    "     {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": [\n",
    "               {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": system_prompt\n",
    "               }\n",
    "          ]\n",
    "     },\n",
    "     {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "               {\n",
    "                    \"type\": \"acv_document_id\",\n",
    "                    \"acv_document_id\": VIDEO_DOCUMENT_ID\n",
    "               },\n",
    "               {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \" \"\n",
    "               }\n",
    "          ]\n",
    "     }\n",
    "],\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "# Send request\n",
    "try:\n",
    "    response = requests.post(GPT4V_ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "# Handle the response as needed (e.g., print or process)\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
