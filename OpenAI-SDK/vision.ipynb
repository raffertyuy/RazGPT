{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# Set environment variables in the .env file.\n",
    "load_dotenv()\n",
    "\n",
    "OPENAI_API_TYPE = os.environ[\"OPENAI_API_TYPE\"]\n",
    "OPENAI_API_VERSION = os.environ[\"OPENAI_API_VERSION\"]\n",
    "OPENAI_API_BASE = os.environ[\"OPENAI_API_BASE\"]\n",
    "OPENAI_API_KEY = os.environ[\"OPENAI_API_KEY\"]\n",
    "\n",
    "OPENAI_DEPLOYMENT_NAME = os.environ[\"OPENAI_DEPLOYMENT_NAME\"]\n",
    "\n",
    "AZURE_AI_VISION_ENDPOINT = os.environ[\"AZURE_AI_VISION_ENDPOINT\"]\n",
    "AZURE_AI_VISION_KEY = os.environ[\"AZURE_AI_VISION_KEY\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Vision Only"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image from Public Web\n",
    "If the image is available via an image URL, the OpenAI SDK may be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason=None, index=0, message=ChatCompletionMessage(content='The image shows four individuals standing against a purple background. From left to right, the first person is wearing a purple t-shirt with dark pants, the second individual is dressed in a dark purple sweater with black pants, the third person is wearing a denim jacket over a black top paired with black pants, and the fourth person is in a light purple long-sleeve shirt with light pink pants. All four individuals appear to be casually posing for the photo.', role='assistant', function_call=None, tool_calls=None), finish_details={'type': 'stop', 'stop': '<|fim_suffix|>'}, content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = OPENAI_API_BASE, \n",
    "  api_key=OPENAI_API_KEY,  \n",
    "  api_version=OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=OPENAI_DEPLOYMENT_NAME,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": \"What is in this image?\"\n",
    "        },\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": \"https://media.wired.com/photos/64ed0bc52da6c6d86e70e575/master/w_1280,c_limit/WI100123_FF_OpenAI_01.jpg\",\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image from Local Machine (REST API)\n",
    "Below is the code sample from OpenAI and Azure OpenAI as of 2024-01-02. It uses the Rest API instead of the SDK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8cUYbQWlME7BVEbiyWiZRKkfUCKPr', 'object': 'chat.completion', 'created': 1704184617, 'model': 'gpt-4', 'prompt_filter_results': [{'prompt_index': 0, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'choices': [{'finish_details': {'type': 'stop', 'stop': '<|fim_suffix|>'}, 'index': 0, 'message': {'role': 'assistant', 'content': 'This is an outdoor image showing a family scene. A person, whose face is blurred, is wearing a white t-shirt and a bright yellow apron, standing in front of a charcoal grill with what appears to be chicken on it. The individual is holding a pair of tongs in one hand and a piece of bread or bun in the other. In the background, two children are playing; one is twirling a hula hoop around her arm while the other appears to be watching. There is a red and white soccer ball on the grass near them. The environment is green with several trees around, suggesting a park or a backyard garden setting.\\n\\nSuggested Tags: Family, Barbecue, Outdoor, Children Playing, Grilling, Park, Picnic, Summer, Family Time\\n\\nPlease note that the faces of the individuals are blurred for privacy.'}, 'content_filter_results': {'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}}}], 'usage': {'prompt_tokens': 1197, 'completion_tokens': 170, 'total_tokens': 1367}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "# Configuration\n",
    "IMAGE_PATH = \"../sampledata/image-barbeque.png\"\n",
    "encoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": OPENAI_API_KEY,\n",
    "}\n",
    "\n",
    "system_prompt = \"\"\"You are an assistant helps the blind. In addition to answering questions, you help the blind understand what is in the images provided by the user.\n",
    "\n",
    "Image outputs should include:\n",
    "- Detailed description\n",
    "- Suggested tags\n",
    "- Key-value pairs (if the image is a form, receipt, invoice, etc.)\"\"\"\n",
    "\n",
    "# Payload for the request\n",
    "payload = {\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": system_prompt\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "GPT4V_ENDPOINT = f\"{OPENAI_API_BASE}/openai/deployments/{OPENAI_DEPLOYMENT_NAME}/extensions/chat/completions?api-version={OPENAI_API_VERSION}\"\n",
    "\n",
    "# Send request\n",
    "try:\n",
    "    response = requests.post(GPT4V_ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "# Handle the response as needed (e.g., print or process)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image from Local Machine (SDK)\n",
    "Below is working code using the OpenAI SDK. This code is not in the official samples as of 2024-01-02."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Choice(finish_reason=None, index=0, message=ChatCompletionMessage(content=\"Description:\\nThe image shows a lively outdoor scene where a man is standing and grilling chicken on a barbecue grill. He is wearing a white T-shirt and a bright yellow apron and is holding barbecue tongs in one hand with what appears to be several pieces of chicken on the grill. His face is blurred for privacy. In the background, there are two children playing with hula hoops in a grassy area with trees around. The child closer to the man is hula hooping, while the younger one is holding a hula hoop and watching, with her face also blurred. Close to the grilling area, there is a deflated red and black soccer ball on the ground.\\n\\nSuggested Tags:\\nOutdoor, Family, BBQ, Grilling, Cooking, Children, Playtime, Recreation, Trees, Nature, Leisure, Activity, Food.\\n\\nPlease note that faces have been intentionally blurred to protect the individuals' privacy.\", role='assistant', function_call=None, tool_calls=None), finish_details={'type': 'stop', 'stop': '<|fim_suffix|>'}, content_filter_results={'hate': {'filtered': False, 'severity': 'safe'}, 'self_harm': {'filtered': False, 'severity': 'safe'}, 'sexual': {'filtered': False, 'severity': 'safe'}, 'violence': {'filtered': False, 'severity': 'safe'}})\n"
     ]
    }
   ],
   "source": [
    "from openai import AzureOpenAI\n",
    "import base64\n",
    "\n",
    "client = AzureOpenAI(\n",
    "  azure_endpoint = OPENAI_API_BASE, \n",
    "  api_key=OPENAI_API_KEY,  \n",
    "  api_version=OPENAI_API_VERSION\n",
    ")\n",
    "\n",
    "# Configuration\n",
    "IMAGE_PATH = \"../sampledata/image-barbeque.png\"\n",
    "encoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')\n",
    "\n",
    "system_prompt = \"\"\"You are an assistant helps the blind. In addition to answering questions, you help the blind understand what is in the images provided by the user.\n",
    "\n",
    "Image outputs should include:\n",
    "- Detailed description\n",
    "- Suggested tags\n",
    "- Key-value pairs (if the image is a form, receipt, invoice, etc.)\"\"\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "  model=OPENAI_DEPLOYMENT_NAME,\n",
    "  messages=[\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": system_prompt\n",
    "        }\n",
    "      ],\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "          },\n",
    "        },\n",
    "      ],\n",
    "    }\n",
    "  ],\n",
    "  max_tokens=300,\n",
    ")\n",
    "\n",
    "print(response.choices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Vision + Azure AI Vision (REST API)\n",
    "To use Azure AI Vision, Azure OpenAI REST API is **required**. This is because it is using the Chat Completions _Extensions_ API, which the official SDK doesn't have."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'id': 'chatcmpl-8cUbceCQC3SQ6WNS2vxA1jfe5x6cL', 'object': 'chat.completion', 'created': 1704184804, 'model': 'gpt-4', 'choices': [{'finish_details': {'type': 'stop', 'stop': '<|fim_suffix|>'}, 'index': 0, 'message': {'role': 'assistant', 'content': \"Description:\\nThe image captures an outdoor setting with a man standing in the foreground, actively grilling chicken on a small charcoal grill. The man is wearing a white t-shirt and a bright yellow apron. His face is blurred for privacy. He appears to be holding a pair of tongs in one hand and a piece of bread or bun in the other. In the background, two children are playing; one is playing with a hula hoop, and the other is standing nearby, possibly waiting for her turn. There's a red soccer ball on the ground beside them. The environment is lush with green grass and trees, suggesting a park or a backyard garden. A pile of dirt or sand and a few scattered fallen branches are also visible in the background.\\n\\nSuggested Tags:\\nOutdoor, Family, BBQ, Grilling, Children, Playing, Park, Garden, Casual, Weekend, Daytime, Recreation, Leisure, Nature.\\n\\nNote:\\nAs requested, the description excludes the blurred face, focusing on the activities and surroundings.\"}, 'enhancements': {'grounding': {'lines': [{'text': \"Description:\\nThe image captures an outdoor setting with a man standing in the foreground, actively grilling chicken on a small charcoal grill. The man is wearing a white t-shirt and a bright yellow apron. His face is blurred for privacy. He appears to be holding a pair of tongs in one hand and a piece of bread or bun in the other. In the background, two children are playing; one is playing with a hula hoop, and the other is standing nearby, possibly waiting for her turn. There's a red soccer ball on the ground beside them. The environment is lush with green grass and trees, suggesting a park or a backyard garden. A pile of dirt or sand and a few scattered fallen branches are also visible in the background.\\n\\nSuggested Tags:\\nOutdoor, Family, BBQ, Grilling, Children, Playing, Park, Garden, Casual, Weekend, Daytime, Recreation, Leisure, Nature.\\n\\nNote:\\nAs requested, the description excludes the blurred face, focusing on the activities and surroundings.\", 'spans': [{'text': 'the man', 'length': 7, 'offset': 143, 'polygon': [{'x': 0.25550001859664917, 'y': 0.15150000154972076}, {'x': 0.5385000705718994, 'y': 0.15150000154972076}, {'x': 0.5385000705718994, 'y': 0.997499942779541}, {'x': 0.25550001859664917, 'y': 0.997499942779541}]}, {'text': 'a white t-shirt', 'length': 15, 'offset': 162, 'polygon': [{'x': 0.26850003004074097, 'y': 0.2944999933242798}, {'x': 0.48450005054473877, 'y': 0.2944999933242798}, {'x': 0.48450005054473877, 'y': 0.5484999418258667}, {'x': 0.26850003004074097, 'y': 0.5484999418258667}]}, {'text': 'a bright yellow apron', 'length': 21, 'offset': 182, 'polygon': [{'x': 0.32249999046325684, 'y': 0.2954999804496765}, {'x': 0.49250003695487976, 'y': 0.2954999804496765}, {'x': 0.49250003695487976, 'y': 0.8374999761581421}, {'x': 0.32249999046325684, 'y': 0.8374999761581421}]}]}], 'status': 'Success'}}}], 'usage': {'prompt_tokens': 1197, 'completion_tokens': 205, 'total_tokens': 1402}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import base64\n",
    "\n",
    "# Configuration\n",
    "IMAGE_PATH = \"../sampledata/image-barbeque.png\"\n",
    "encoded_image = base64.b64encode(open(IMAGE_PATH, 'rb').read()).decode('ascii')\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": OPENAI_API_KEY,\n",
    "}\n",
    "\n",
    "system_prompt = \"\"\"You are an assistant helps the blind. In addition to answering questions, you help the blind understand what is in the images provided by the user.\n",
    "\n",
    "Image outputs should include:\n",
    "- Detailed description\n",
    "- Suggested tags\n",
    "- Key-value pairs (if the image is a form, receipt, invoice, etc.)\"\"\"\n",
    "\n",
    "# Payload for the request\n",
    "payload = {\n",
    "  \"enhancements\": {\n",
    "    \"ocr\": {\n",
    "      \"enabled\": True\n",
    "    },\n",
    "    \"grounding\": {\n",
    "      \"enabled\": True\n",
    "    }\n",
    "  },\n",
    "  \"dataSources\": [\n",
    "    {\n",
    "      \"type\" : \"AzureComputerVision\",\n",
    "      \"parameters\" : {\n",
    "        \"endpoint\" : AZURE_AI_VISION_ENDPOINT,\n",
    "        \"key\" : AZURE_AI_VISION_KEY\n",
    "      }\n",
    "    }\n",
    "  ],\n",
    "  \"messages\": [\n",
    "    {\n",
    "      \"role\": \"system\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"text\",\n",
    "          \"text\": system_prompt\n",
    "        }\n",
    "      ]\n",
    "    },\n",
    "    {\n",
    "      \"role\": \"user\",\n",
    "      \"content\": [\n",
    "        {\n",
    "          \"type\": \"image_url\",\n",
    "          \"image_url\": {\n",
    "            \"url\": f\"data:image/jpeg;base64,{encoded_image}\"\n",
    "          }\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  ],\n",
    "  \"temperature\": 0.7,\n",
    "  \"top_p\": 0.95,\n",
    "  \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "GPT4V_ENDPOINT = f\"{OPENAI_API_BASE}/openai/deployments/{OPENAI_DEPLOYMENT_NAME}/extensions/chat/completions?api-version={OPENAI_API_VERSION}\"\n",
    "\n",
    "# Send request\n",
    "try:\n",
    "    response = requests.post(GPT4V_ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "# Handle the response as needed (e.g., print or process)\n",
    "print(response.json())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "201 {\"name\":\"visionindex01\",\"userData\":{},\"features\":[{\"name\":\"vision\",\"modelVersion\":\"2023-05-31\",\"domain\":\"surveillance\"}],\"eTag\":\"\\\"2c98b25f876a41f0a181a675c5945420\\\"\",\"createdDateTime\":\"2024-01-11T05:58:39.2433630Z\",\"lastModifiedDateTime\":\"2024-01-11T05:58:39.2433630Z\"}\n",
      "202 {\"name\":\"my-ingestion\",\"state\":\"Running\",\"batchName\":\"91bdc888-ced8-4164-8f66-bda91cb14741\",\"createdDateTime\":\"2024-01-11T05:58:40.6027401Z\",\"lastModifiedDateTime\":\"2024-01-11T05:58:40.9933680Z\"}\n",
      "{'value': [{'name': 'my-ingestion', 'state': 'Completed', 'batchName': '91bdc888-ced8-4164-8f66-bda91cb14741', 'createdDateTime': '2024-01-11T05:58:40.6027401Z', 'lastModifiedDateTime': '2024-01-11T05:59:05.8527613Z'}]}\n",
      "Ingestion completed.\n",
      "{'id': 'chatcmpl-8fiOEhCPexw0WJvLYpN3HDRm20ZfK', 'object': 'chat.completion', 'created': 1704952774, 'model': 'gpt-4', 'choices': [{'finish_details': {'type': 'stop', 'stop': '<|fim_suffix|>'}, 'index': 0, 'message': {'role': 'assistant', 'content': 'The images provided are from various timestamps of a video that appears to be set in an industrial or manufacturing environment. \\n\\n1. Timestamp 00:00:02 - The image shows a wide view of a manufacturing workshop with various machines, such as CNC machines and lathes, aligned in rows. There are no people visible in the frame.\\n\\n2. Timestamp 00:00:16 - A worker wearing a high-visibility vest and a hard hat is visible walking through the aisle between the machinery.\\n\\n3. Timestamp 00:00:27 to 00:00:33 - These images show a different angle of the workshop with a worker pushing a manual pallet jack across the room.\\n\\n4. Timestamp 00:01:10 to 00:02:02 - A series of images showing a different section of the workshop, this time with workers interacting with CNC machines and work benches with various tools and metal parts. One worker is seen inspecting a piece while another is walking down the aisle.\\n\\n5. Timestamp 00:02:08 - A worker is seen sitting on a chair, seemingly taking a break from work.\\n\\n6. Timestamp 00:02:15 - The scene shifts to an outdoor area showing a loading dock with dumpsters, a parked truck, and a worker walking in the distance.\\n\\n7. Timestamp 00:02:28 to 00:03:03 - Back inside, we see a section of the workshop with a worker wearing a high-visibility vest working near a bench with blueprints and another one carrying a box.\\n\\n8. Timestamp 00:03:07 - The scene shows a worker in a different part of the workshop with various tools and workbenches.\\n\\n9. Timestamp 00:03:23 to 00:03:24 - The final images return to the outdoor loading dock area, now with a forklift in operation and workers around.\\n\\nSuggested tags for these images include: manufacturing, workshop, machinery, workers, industrial, CNC machines, tools, workbenches, loading dock, forklift, safety gear, high-visibility vest, hard hat.\\n\\nThese images do not appear to contain any forms, receipts, or invoices that would require key-value pair extraction.'}}], 'usage': {'prompt_tokens': 2039, 'completion_tokens': 451, 'total_tokens': 2490}}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "\n",
    "# Configuration\n",
    "GPT4V_ENDPOINT = f\"{OPENAI_API_BASE}/openai/deployments/{OPENAI_DEPLOYMENT_NAME}/extensions/chat/completions?api-version={OPENAI_API_VERSION}\"\n",
    "\n",
    "## ingest the video\n",
    "VIDEO_FILE_SAS_URL = \"https://raztypestore.blob.core.windows.net/temp/video-factory.mp4?sv=2023-01-03&st=2024-01-11T05%3A42%3A07Z&se=2024-03-30T05%3A42%3A00Z&sr=b&sp=r&sig=sArMtPI4ICs95MQrhvYGP5oAFB8DjpjWtJNwEgqPzYE%3D\"\n",
    "VIDEO_INDEX_NAME = \"visionindex01\" # this needs to be unique, append number. To delete old indices, use the REST API https://learn.microsoft.com/en-us/azure/ai-services/computer-vision/reference-video-search\n",
    "VIDEO_DOCUMENT_ID = \"AOAIChatDocument\"\n",
    "\n",
    "def create_video_index(vision_api_endpoint, vision_api_key, index_name):\n",
    "    url = f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}?api-version=2023-05-01-preview\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key, \"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        \"features\": [\n",
    "            {\"name\": \"vision\", \"domain\": \"surveillance\"}\n",
    "        ]\n",
    "    }\n",
    "    response = requests.put(url, headers=headers, json=data)\n",
    "    return response\n",
    "\n",
    "def add_video_to_index(vision_api_endpoint, vision_api_key, index_name, video_url, video_id):\n",
    "    url = f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}/ingestions/my-ingestion?api-version=2023-05-01-preview\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key, \"Content-Type\": \"application/json\"}\n",
    "    data = {\n",
    "        'videos': [{'mode': 'add', 'documentId': video_id, 'documentUrl': video_url}]\n",
    "    }\n",
    "    response = requests.put(url, headers=headers, json=data)\n",
    "    return response\n",
    "\n",
    "def wait_for_ingestion_completion(vision_api_endpoint, vision_api_key, index_name, max_retries=30):\n",
    "    url = f\"{vision_api_endpoint}/computervision/retrieval/indexes/{index_name}/ingestions?api-version=2023-05-01-preview\"\n",
    "    headers = {\"Ocp-Apim-Subscription-Key\": vision_api_key}\n",
    "    retries = 0\n",
    "    while retries < max_retries:\n",
    "        time.sleep(10)\n",
    "        response = requests.get(url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            state_data = response.json()\n",
    "            if state_data['value'][0]['state'] == 'Completed':\n",
    "                print(state_data)\n",
    "                print('Ingestion completed.')\n",
    "                return True\n",
    "            elif state_data['value'][0]['state'] == 'Failed':\n",
    "                print(state_data)\n",
    "                print('Ingestion failed.')\n",
    "                return False\n",
    "        retries += 1\n",
    "    return False\n",
    "\n",
    "\n",
    "# Step 1: Create an Index\n",
    "response = create_video_index(AZURE_AI_VISION_ENDPOINT, AZURE_AI_VISION_KEY, VIDEO_INDEX_NAME)\n",
    "print(response.status_code, response.text)\n",
    "\n",
    "# Step 2: Add a video file to the index\n",
    "response = add_video_to_index(AZURE_AI_VISION_ENDPOINT, AZURE_AI_VISION_KEY, VIDEO_INDEX_NAME, VIDEO_FILE_SAS_URL, VIDEO_DOCUMENT_ID)\n",
    "print(response.status_code, response.text)\n",
    "\n",
    "# Step 3: Wait for ingestion to complete\n",
    "if not wait_for_ingestion_completion(AZURE_AI_VISION_ENDPOINT, AZURE_AI_VISION_KEY, VIDEO_INDEX_NAME):\n",
    "    print(\"Ingestion did not complete within the expected time.\")\n",
    "\n",
    "\n",
    "## Chat with GPT-4V\n",
    "\n",
    "headers = {\n",
    "    \"Content-Type\": \"application/json\",\n",
    "    \"api-key\": OPENAI_API_KEY,\n",
    "}\n",
    "\n",
    "system_prompt = \"\"\"You are an assistant helps the blind. In addition to answering questions, you help the blind understand what is in the images provided by the user.\n",
    "\n",
    "Image outputs should include:\n",
    "- Detailed description\n",
    "- Suggested tags\n",
    "- Key-value pairs (if the image is a form, receipt, invoice, etc.)\"\"\"\n",
    "\n",
    "# Payload for the request\n",
    "payload = {\n",
    "    \"dataSources\": [\n",
    "        {\n",
    "            \"type\": \"AzureComputerVisionVideoIndex\",\n",
    "            \"parameters\": {\n",
    "                \"computerVisionBaseUrl\": f\"{AZURE_AI_VISION_ENDPOINT}/computervision\",\n",
    "                \"computerVisionApiKey\": AZURE_AI_VISION_KEY,\n",
    "                \"indexName\": VIDEO_INDEX_NAME,\n",
    "                \"videoUrls\": [VIDEO_FILE_SAS_URL]\n",
    "            }\n",
    "        }\n",
    "    ],\n",
    "    \"enhancements\": {\n",
    "        \"video\": {\n",
    "            \"enabled\": True\n",
    "        }\n",
    "    },\n",
    "    \"messages\": [\n",
    "     {\n",
    "          \"role\": \"system\",\n",
    "          \"content\": [\n",
    "               {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": system_prompt\n",
    "               }\n",
    "          ]\n",
    "     },\n",
    "     {\n",
    "          \"role\": \"user\",\n",
    "          \"content\": [\n",
    "               {\n",
    "                    \"type\": \"acv_document_id\",\n",
    "                    \"acv_document_id\": VIDEO_DOCUMENT_ID\n",
    "               },\n",
    "               {\n",
    "                    \"type\": \"text\",\n",
    "                    \"text\": \" \"\n",
    "               }\n",
    "          ]\n",
    "     }\n",
    "],\n",
    "    \"temperature\": 0.7,\n",
    "    \"top_p\": 0.95,\n",
    "    \"max_tokens\": 800\n",
    "}\n",
    "\n",
    "# Send request\n",
    "try:\n",
    "    response = requests.post(GPT4V_ENDPOINT, headers=headers, json=payload)\n",
    "    response.raise_for_status()  # Will raise an HTTPError if the HTTP request returned an unsuccessful status code\n",
    "except requests.RequestException as e:\n",
    "    raise SystemExit(f\"Failed to make the request. Error: {e}\")\n",
    "\n",
    "# Handle the response as needed (e.g., print or process)\n",
    "print(response.json())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
